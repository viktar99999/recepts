{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c67ce430-ea19-4fff-a2a6-8fbfd16a0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be72ded6-0998-4866-b179-993300f6b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b67f5d7-7e66-43b1-995c-a06a0f03d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906dcce4-00de-487c-9021-9afc5b71c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d262c0ca-48f0-4eea-90d1-0bedb4f4fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5befde6e-0d75-4bf3-bcfa-63599c5e7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ef477d-bb09-457e-b138-c7ca509eef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c28275e-e728-4fc8-a78d-923db6d8b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5ca868-6b19-4552-a9ec-d4fb8481d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9bc907a-9e91-4093-87a0-c13860795297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058201e3-8356-4d99-ae5f-4c04fb4cacdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233d3b7a-b9ed-44fa-a6d6-464de90def7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b7c98d7-c357-4d7b-a2df-265efbbcf8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34dd4def-c801-4dda-bcd6-50ede5207f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b26a97-4100-4c27-a561-362c77ca54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25fe9935-31f6-45fc-b0db-055b91745f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8ed755a-d887-400a-b68f-283db776666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python_libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d29c953-0871-42ff-b3e2-11b6d5e9a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4866cca7-a9e2-4be7-b0e5-09841eefb93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf5e2a7-12f0-4b77-a66a-75217eea18b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7641579564f0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "675a6a05-cc7a-4f02-94ee-d9e05b297984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be02316-8b67-4cb0-982b-f5fa01d034c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('recepts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4330da59-8737-472c-8847-fa036e82b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0378536a-44e6-4b4a-a6e9-9966fc706486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipecategory</th>\n",
       "      <th>keywords_1</th>\n",
       "      <th>keywords_2</th>\n",
       "      <th>keywords_3</th>\n",
       "      <th>keywords_4</th>\n",
       "      <th>keywords_5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>aggregatedrating</th>\n",
       "      <th>reviewcount</th>\n",
       "      <th>calories</th>\n",
       "      <th>fatcontent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170.9</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>52</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1110.7</td>\n",
       "      <td>58.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>311.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>536.1</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>103.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   recipecategory  keywords_1  keywords_2  keywords_3  keywords_4  keywords_5  \\\n",
       "0              26           2           9           3           6          10   \n",
       "1              15          52          12           4           1          10   \n",
       "2               8          10           6           5           3           1   \n",
       "3              76          17           2           3          14           1   \n",
       "4               4          10           7           3           6          19   \n",
       "\n",
       "     0    1    2     3  ...    6    7    8    9    10    11  aggregatedrating  \\\n",
       "0  2.0  1.0  1.0   4.0  ...  8.0  2.0  1.0  1.0   4.0   4.0               4.5   \n",
       "1  3.0  2.0  2.0   1.0  ...  3.0  3.0  2.0  2.0   1.0   1.0               3.0   \n",
       "2  1.0  1.0  1.0  12.0  ...  2.0  1.0  1.0  1.0  12.0  12.0               4.5   \n",
       "3  2.0  1.0  2.0  46.0  ...  1.0  2.0  1.0  2.0  46.0  46.0               4.5   \n",
       "4  1.0  1.0  1.0  12.0  ...  1.0  1.0  1.0  1.0  12.0  12.0               4.5   \n",
       "\n",
       "   reviewcount  calories  fatcontent  \n",
       "0          4.0     170.9         2.5  \n",
       "1          1.0    1110.7        58.8  \n",
       "2         10.0     311.1         0.2  \n",
       "3          2.0     536.1        24.0  \n",
       "4         11.0     103.6         0.4  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eddfbfef-15a8-4f3b-8abf-ad2adb4e0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head() recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d23a707-0122-42f4-a09a-9d47cb197018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recipecategory      0\n",
       "keywords_1          0\n",
       "keywords_2          0\n",
       "keywords_3          0\n",
       "keywords_4          0\n",
       "keywords_5          0\n",
       "0                   0\n",
       "1                   0\n",
       "2                   0\n",
       "3                   0\n",
       "4                   0\n",
       "5                   0\n",
       "6                   0\n",
       "7                   0\n",
       "8                   0\n",
       "9                   0\n",
       "10                  0\n",
       "11                  0\n",
       "aggregatedrating    0\n",
       "reviewcount         0\n",
       "calories            0\n",
       "fatcontent          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0da27a3-6c12-4a59-8a68-8f2a16b846e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show isna().sum() recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18b49d19-03e9-4efd-a8ba-2c3b9b3e70f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipecategory        int64\n",
      "keywords_1            int64\n",
      "keywords_2            int64\n",
      "keywords_3            int64\n",
      "keywords_4            int64\n",
      "keywords_5            int64\n",
      "0                   float64\n",
      "1                   float64\n",
      "2                   float64\n",
      "3                   float64\n",
      "4                   float64\n",
      "5                   float64\n",
      "6                   float64\n",
      "7                   float64\n",
      "8                   float64\n",
      "9                   float64\n",
      "10                  float64\n",
      "11                  float64\n",
      "aggregatedrating    float64\n",
      "reviewcount         float64\n",
      "calories            float64\n",
      "fatcontent          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26c19db7-aee8-4279-b5c4-b81cb5e87312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show dtypes() recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd803f19-3f1b-4ad1-985a-4c8022e388b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 522517 entries, 0 to 522516\n",
      "Data columns (total 22 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   recipecategory    522517 non-null  int64  \n",
      " 1   keywords_1        522517 non-null  int64  \n",
      " 2   keywords_2        522517 non-null  int64  \n",
      " 3   keywords_3        522517 non-null  int64  \n",
      " 4   keywords_4        522517 non-null  int64  \n",
      " 5   keywords_5        522517 non-null  int64  \n",
      " 6   0                 522517 non-null  float64\n",
      " 7   1                 522517 non-null  float64\n",
      " 8   2                 522517 non-null  float64\n",
      " 9   3                 522517 non-null  float64\n",
      " 10  4                 522517 non-null  float64\n",
      " 11  5                 522517 non-null  float64\n",
      " 12  6                 522517 non-null  float64\n",
      " 13  7                 522517 non-null  float64\n",
      " 14  8                 522517 non-null  float64\n",
      " 15  9                 522517 non-null  float64\n",
      " 16  10                522517 non-null  float64\n",
      " 17  11                522517 non-null  float64\n",
      " 18  aggregatedrating  522517 non-null  float64\n",
      " 19  reviewcount       522517 non-null  float64\n",
      " 20  calories          522517 non-null  float64\n",
      " 21  fatcontent        522517 non-null  float64\n",
      "dtypes: float64(16), int64(6)\n",
      "memory usage: 87.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7d66ee8-590b-43dc-add1-71c77db0a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show info() recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49af6c43-5eee-46ce-9e80-ded4edde36a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522517, 22)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5fe7fe2-8133-4a0e-b4b4-60cf2f20dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39e1e8e7-3587-4ccf-b47e-eae5476b7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['fatcontent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "723688d7-ceba-42e9-9512-ff8920532506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show target(Y) recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6eaad6b8-4939-4d7e-85c7-5207d1e652d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:21]\n",
    "y = df.iloc[:, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50e29f67-abef-4645-8017-43e4cc3e0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show target(y) recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba888466-0de0-41c9-8936-1e968a2bbb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipecategory</th>\n",
       "      <th>keywords_1</th>\n",
       "      <th>keywords_2</th>\n",
       "      <th>keywords_3</th>\n",
       "      <th>keywords_4</th>\n",
       "      <th>keywords_5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>aggregatedrating</th>\n",
       "      <th>reviewcount</th>\n",
       "      <th>calories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>170.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>52</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1110.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>311.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>536.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>103.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   recipecategory  keywords_1  keywords_2  keywords_3  keywords_4  keywords_5  \\\n",
       "0              26           2           9           3           6          10   \n",
       "1              15          52          12           4           1          10   \n",
       "2               8          10           6           5           3           1   \n",
       "3              76          17           2           3          14           1   \n",
       "4               4          10           7           3           6          19   \n",
       "\n",
       "     0    1    2     3  ...    5    6    7    8    9    10    11  \\\n",
       "0  2.0  1.0  1.0   4.0  ...  1.0  8.0  2.0  1.0  1.0   4.0   4.0   \n",
       "1  3.0  2.0  2.0   1.0  ...  2.0  3.0  3.0  2.0  2.0   1.0   1.0   \n",
       "2  1.0  1.0  1.0  12.0  ...  1.0  2.0  1.0  1.0  1.0  12.0  12.0   \n",
       "3  2.0  1.0  2.0  46.0  ...  2.0  1.0  2.0  1.0  2.0  46.0  46.0   \n",
       "4  1.0  1.0  1.0  12.0  ...  1.0  1.0  1.0  1.0  1.0  12.0  12.0   \n",
       "\n",
       "   aggregatedrating  reviewcount  calories  \n",
       "0               4.5          4.0     170.9  \n",
       "1               3.0          1.0    1110.7  \n",
       "2               4.5         10.0     311.1  \n",
       "3               4.5          2.0     536.1  \n",
       "4               4.5         11.0     103.6  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acee4f56-9ae7-4775-9115-0673bf8f588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head() recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "321300af-da34-4f2a-bc0d-319693f71617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2.5\n",
      "1    58.8\n",
      "2     0.2\n",
      "3    24.0\n",
      "4     0.4\n",
      "Name: fatcontent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8beb7622-0b4a-42c3-a07a-07d7bb4ea2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show target(y) recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7276560-085b-4dec-babc-ed03fc711649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       recipecategory     keywords_1     keywords_2     keywords_3  \\\n",
      "count   522517.000000  522517.000000  522517.000000  522517.000000   \n",
      "mean        16.309261      16.171478      13.838792       9.577702   \n",
      "std         22.785018      20.981579      17.691402      12.721010   \n",
      "min          1.000000       1.000000       1.000000       1.000000   \n",
      "25%          1.000000       4.000000       3.000000       2.000000   \n",
      "50%          7.000000       8.000000       7.000000       5.000000   \n",
      "75%         21.000000      19.000000      17.000000      10.000000   \n",
      "max        208.000000     195.000000     167.000000     141.000000   \n",
      "\n",
      "          keywords_4     keywords_5              0              1  \\\n",
      "count  522517.000000  522517.000000  522517.000000  522517.000000   \n",
      "mean        9.005596       7.506607       6.531346       6.590226   \n",
      "std        10.283178       7.904510      39.700560      41.644158   \n",
      "min         1.000000       1.000000       0.250000       0.250000   \n",
      "25%         2.000000       2.000000       1.000000       1.000000   \n",
      "50%         5.000000       5.000000       1.000000       1.000000   \n",
      "75%        12.000000      10.000000       2.000000       2.000000   \n",
      "max       122.000000      94.000000    2000.000000    2500.000000   \n",
      "\n",
      "                   2              3  ...              5              6  \\\n",
      "count  522517.000000  522517.000000  ...  522517.000000  522517.000000   \n",
      "mean        6.468904      14.338731  ...       6.508440       6.472277   \n",
      "std        41.476019      71.710402  ...      39.254083      39.064102   \n",
      "min         0.250000       0.250000  ...       0.125000       0.250000   \n",
      "25%         1.000000       1.000000  ...       1.000000       1.000000   \n",
      "50%         1.000000       2.000000  ...       1.000000       1.000000   \n",
      "75%         2.000000       4.000000  ...       2.000000       2.000000   \n",
      "max      5742.000000    3750.000000  ...    2500.000000    2147.000000   \n",
      "\n",
      "                   7              8              9             10  \\\n",
      "count  522517.000000  522517.000000  522517.000000  522517.000000   \n",
      "mean        6.531346       6.590226       6.468904      14.338731   \n",
      "std        39.700560      41.644158      41.476019      71.710402   \n",
      "min         0.250000       0.250000       0.250000       0.250000   \n",
      "25%         1.000000       1.000000       1.000000       1.000000   \n",
      "50%         1.000000       1.000000       1.000000       2.000000   \n",
      "75%         2.000000       2.000000       2.000000       4.000000   \n",
      "max      2000.000000    2500.000000    5742.000000    3750.000000   \n",
      "\n",
      "                  11  aggregatedrating    reviewcount       calories  \n",
      "count  522517.000000     522517.000000  522517.000000  522517.000000  \n",
      "mean       14.335309          4.630922       5.375949     484.438580  \n",
      "std        71.721677          0.640397      20.873106    1397.116649  \n",
      "min         0.250000          1.000000       1.000000       0.000000  \n",
      "25%         1.000000          4.500000       1.000000     174.200000  \n",
      "50%         2.000000          5.000000       2.000000     317.100000  \n",
      "75%         4.000000          5.000000       4.000000     529.100000  \n",
      "max      3750.000000          5.000000    3063.000000  612854.600000  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "efa44108-d8c1-480c-bbae-73789ceedc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show describe() recepts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4111d7a0-f6f0-42fb-9219-7580229bee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1aa85b09-a6af-47ca-bcea-45edaa813aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = 0.7, test = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9551ff1b-caec-43b4-8775-7a7f0c04be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63ac4230-5c82-46f2-9cc9-b2a001768838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "48f9155f-29f5-48d8-9946-6c6c4b5c5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a742411-c010-4c6a-9236-126d169f4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca346957-a403-48c8-90f2-988a3e1e714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da94130e-aa26-4a9f-8c7b-e3c0e1139443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6badece8-c2f4-414b-bd57-97c67b8f7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a0f57c7-f1d3-4a0e-bd46-a31b7a24851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b0d6fd6-0fe4-4dbc-a9f2-a3a64925ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=100,\n",
    "\n",
    "                                  criterion='poisson', max_depth=14, max_features=9, n_jobs = 8, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c2be109-cc90-462f-9dbe-f83293c161ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators=100, criterion='poisson', max_depth=14, max_features=9, n_jobs = 8, random_state=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "839a8118-7f66-4abe-a901-023ed055f94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(criterion=&#x27;poisson&#x27;, max_depth=14, max_features=9,\n",
       "                      n_jobs=8, random_state=40)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomForestRegressor<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">?<span>Documentation for RandomForestRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestRegressor(criterion=&#x27;poisson&#x27;, max_depth=14, max_features=9,\n",
       "                      n_jobs=8, random_state=40)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(criterion='poisson', max_depth=14, max_features=9,\n",
       "                      n_jobs=8, random_state=40)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "638e500e-75cb-4d96-b13d-d7effd1424f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a054bb2-a8ce-430d-b9cf-4f09c3509a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12a21d45-7499-4aad-8da1-6ef327e3c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aacaebe5-f532-439d-a69e-32de90c007f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7522999277910686"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12be9705-054f-43f8-81b5-f05b5b6eb9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.R2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8e704c2-d219-4913-974e-5619e6f11a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7522999277910686\n"
     ]
    }
   ],
   "source": [
    "print(r2(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09d2b6c7-0a8e-492a-a409-6e58481d9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.R2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "948c18af-3377-4f5c-ab9d-312e2eea6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "818bf748-2d1c-4613-9584-e0c762b2e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f49fdf14-7021-4025-9618-79b7a9c4e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e4d241e4-76dc-4968-a8a3-53ad54e199fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b20a2a05-5ee1-4c9b-8826-d2ce8b999c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.Tensor(np.array(X_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62bbd6bc-d7ef-4592-a7cf-61f55a97c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_train tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e48442c-66a4-4815-ab18-c1c007804776",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.Tensor(np.array(X_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30f98073-d538-4cd3-92ca-c6129367ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X_test tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58ce53f3-f503-47a7-a7ca-f85cae3edd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.Tensor(np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f355fd0-cbc5-4e72-aea2-1070cfe39616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform y_train tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e90b8a2e-46d0-4845-83f9-da80930de412",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor = torch.Tensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6169fbcc-f71f-4f41-b299-3e5e5a04299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform y_test tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70c61d14-0fd7-4ba6-8586-893561db7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data, n_features = X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3bb59ff-8388-4e2c-a411-341134e1c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() n_data, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03a82cd7-efc9-4087-b768-87b9acbf5e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365761\n"
     ]
    }
   ],
   "source": [
    "print(n_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d05fa23-44f5-4aad-8af9-1217420d3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "18803635-6794-4cc6-8724-ba4e78f531c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05239dfc-5f5f-4644-8542-f2fee3bdf070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show shape() n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4e22ffa8-2887-4fe1-9f93-21bd0f728979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(mean, target):\n",
    "    \"\"\"function loss\"\"\"\n",
    "    return mean(F.l1_loss(input, target, reduction=\"none\") / target) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c567ed6-5a39-4d0b-b59b-7b795712668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function loss\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00088322-51d6-47db-a796-475cda3eb4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8742df23-ad52-432e-9e8d-265697c7b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2bd8524-25dc-4a73-a5e3-c28ca36e0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_func_1 = [loss_func, loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "441e413b-1708-45ed-81ba-9e578d61a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0859c0fe-4fe4-47ef-a370-f4f4b4c59ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name = [\"MSE\", \"LOSS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "430a2c3f-9fc5-4822-aed8-9472d569683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "321aca31-15e0-473f-991b-fa02d736c366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(models, train_data, test_data, models_name):\n",
    "    \"\"\"function print_metrics\"\"\"\n",
    "    results = np.ones(2 * len(models), len(metrics_func_1))\n",
    "    models_name = []\n",
    "    for model in models_name:\n",
    "        models_name.extend([model + \"Train\", model + \"Test\"])\n",
    "    for row, sample in enumerate([train_data, test_data]):\n",
    "        results[row + sample * 2] = evaluate(models, metrics_func_1, sample[0], sample[1])\n",
    "        results = pd.DataFrame(results, columns=metrics_name, index=models_name)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "04f1936f-775f-4d78-b1ae-7759ab4fafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function print_metrics\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "16fe6cf6-13f2-4273-ac1c-a877ad443a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = (X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "45f10236-cc77-4d72-9033-869594ab0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f06c008-c50e-474e-8748-3da2e131ac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = (X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "58856dc1-d992-4350-b4f7-e9980cfb121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f689725c-b910-4f6f-94a1-0cf2232540b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_sklearn = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e65003a8-82cb-4768-93d2-5278888a3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ab571a33-46e0-4899-ab5a-49607ec92265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_sklearn.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4325886a-329e-429e-8cd3-ad24f2dce16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SkLearn.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7eae2d01-656d-4eb5-9267-95fde917b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = [model_lr_sklearn.predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bfd54067-9fae-4653-abef-edd983784742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lr_sklearn.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0fb8ed1d-abe4-445b-82ce-8f3ae583b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name = [\"MSE\", \"LOSS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "12975044-d9c4-48bf-a27b-acd0a7d971e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "914693d2-be13-46e5-8210-6240608e1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name_1 = [\"LOSS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a4d6266d-d6f8-45ad-b639-ecb2014ac253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "95fba304-4995-4967-abe1-4ef70033f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = nn.Sequential(\n",
    "    nn.Linear(in_features=n_features,\n",
    "    out_features=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "86b68405-77f3-4119-8dc8-f6d748c8c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bfb23e96-78ad-478b-ab5f-468249eda8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=21, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c57a4ed3-4814-4dfa-abd9-65ff59bbd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1def8d99-4a98-47e3-856b-a6b694a385d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name_1 = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "14eb1265-0408-4502-9509-feb15afb73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f087017e-5b7e-4cc7-a6ec-63dae0ecff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = optim.SGD(params=model_lr.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "602c38ae-1dce-47d2-8741-f95a5e140599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim model_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "68f6cdac-6c2d-47f0-9844-7e8b280078af",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_LR = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "caf4f39c-5383-4aeb-97a6-4bf9ee5c2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be4b4b17-7bc9-425e-8fc9-be0c114317b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_LR = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "56840b74-32d1-4322-b6bd-bfbf06d001c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c79e3d39-0ad0-4f69-949b-6c303666e351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [06:24<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.trange(EPOCHS_LR):\n",
    "    for i in range((n_data - 1) // BATCH_SIZE_LR + 1):\n",
    "        start_i = i * BATCH_SIZE_LR\n",
    "        end_i = start_i + BATCH_SIZE_LR\n",
    "        Xb = X_train_tensor[start_i: end_i]\n",
    "        yb = y_train_tensor[start_i: end_i]\n",
    "        pred = yb\n",
    "        loss_1 = loss_func(pred, yb + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "af825b22-ee4f-4c80-8444-21baa7c07bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm.trange(EPOCHS_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "faf03c24-8678-427f-8fb7-e2280af2b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.14267765  0.039095    0.79884574 ... -0.98421443 -0.01879476\n",
      "  -0.09096936]\n",
      " [ 0.64735879 -0.48534494 -0.66910215 ...  0.57634728 -0.20382646\n",
      "  -0.04115568]\n",
      " [-0.67204021  3.51946916 -0.21742588 ...  0.57634728 -0.20382646\n",
      "  -0.20807853]\n",
      " ...\n",
      " [ 0.11959919 -0.67605037 -0.49972355 ...  0.57634728 -0.01879476\n",
      "  -0.23577368]\n",
      " [-0.18826058 -0.48534494 -0.21742588 ...  0.57634728 -0.06505269\n",
      "   0.20665959]\n",
      " [ 0.16357915 -0.67605037 -0.49972355 ...  0.57634728  1.8777802\n",
      "  -0.25087444]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "15121543-943a-4cf9-b88f-4c00f55d5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ba6e9567-3531-4d2f-9a6e-2521ebd9eee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360511    18.0\n",
      "392362     5.3\n",
      "441510     0.3\n",
      "106802     1.0\n",
      "53        22.2\n",
      "          ... \n",
      "326281    26.6\n",
      "64228      0.9\n",
      "340331     2.9\n",
      "1465      53.4\n",
      "40621      4.8\n",
      "Name: fatcontent, Length: 156756, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e3c50566-d90e-4363-b398-54f3540bce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2ddf938e-b543-4159-87af-bba3f4f8e514",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(BATCH_SIZE_LR, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eabfe6bb-69fb-44d3-b2a1-9f58a014a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "65399ed1-12f8-4cc7-98c3-de38f561ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "406f7200-433d-48cd-a9e6-4180c339aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7c817fd6-6a8e-415d-9b90-a8260c2ae5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.6672216653823853\n",
      "epoch:  1  loss:  0.6600652933120728\n",
      "epoch:  2  loss:  0.6530008912086487\n",
      "epoch:  3  loss:  0.6460270881652832\n",
      "epoch:  4  loss:  0.6391427516937256\n",
      "epoch:  5  loss:  0.6323467493057251\n",
      "epoch:  6  loss:  0.6256378293037415\n",
      "epoch:  7  loss:  0.6190148591995239\n",
      "epoch:  8  loss:  0.612476646900177\n",
      "epoch:  9  loss:  0.6060222387313843\n",
      "epoch:  10  loss:  0.5996501445770264\n",
      "epoch:  11  loss:  0.5933597087860107\n",
      "epoch:  12  loss:  0.5871496200561523\n",
      "epoch:  13  loss:  0.5810188055038452\n",
      "epoch:  14  loss:  0.5749661922454834\n",
      "epoch:  15  loss:  0.5689908266067505\n",
      "epoch:  16  loss:  0.5630916357040405\n",
      "epoch:  17  loss:  0.5572676658630371\n",
      "epoch:  18  loss:  0.5515178442001343\n",
      "epoch:  19  loss:  0.5458412170410156\n",
      "epoch:  20  loss:  0.54023677110672\n",
      "epoch:  21  loss:  0.5347035527229309\n",
      "epoch:  22  loss:  0.5292407274246216\n",
      "epoch:  23  loss:  0.523847222328186\n",
      "epoch:  24  loss:  0.5185221433639526\n",
      "epoch:  25  loss:  0.5132645964622498\n",
      "epoch:  26  loss:  0.5080737471580505\n",
      "epoch:  27  loss:  0.5029486417770386\n",
      "epoch:  28  loss:  0.497888445854187\n",
      "epoch:  29  loss:  0.4928922951221466\n",
      "epoch:  30  loss:  0.4879592955112457\n",
      "epoch:  31  loss:  0.4830886125564575\n",
      "epoch:  32  loss:  0.4782794713973999\n",
      "epoch:  33  loss:  0.473531037569046\n",
      "epoch:  34  loss:  0.4688425064086914\n",
      "epoch:  35  loss:  0.4642130732536316\n",
      "epoch:  36  loss:  0.4596419930458069\n",
      "epoch:  37  loss:  0.45512843132019043\n",
      "epoch:  38  loss:  0.45067161321640015\n",
      "epoch:  39  loss:  0.4462708830833435\n",
      "epoch:  40  loss:  0.44192540645599365\n",
      "epoch:  41  loss:  0.4376344084739685\n",
      "epoch:  42  loss:  0.4333973526954651\n",
      "epoch:  43  loss:  0.42921340465545654\n",
      "epoch:  44  loss:  0.42508190870285034\n",
      "epoch:  45  loss:  0.4210020899772644\n",
      "epoch:  46  loss:  0.4169733226299286\n",
      "epoch:  47  loss:  0.412994921207428\n",
      "epoch:  48  loss:  0.40906620025634766\n",
      "epoch:  49  loss:  0.40518662333488464\n",
      "epoch:  50  loss:  0.4013553559780121\n",
      "epoch:  51  loss:  0.3975719213485718\n",
      "epoch:  52  loss:  0.39383551478385925\n",
      "epoch:  53  loss:  0.3901457190513611\n",
      "epoch:  54  loss:  0.3865017294883728\n",
      "epoch:  55  loss:  0.3829030692577362\n",
      "epoch:  56  loss:  0.37934908270835876\n",
      "epoch:  57  loss:  0.3758392333984375\n",
      "epoch:  58  loss:  0.3723728656768799\n",
      "epoch:  59  loss:  0.36894941329956055\n",
      "epoch:  60  loss:  0.3655683994293213\n",
      "epoch:  61  loss:  0.3622291386127472\n",
      "epoch:  62  loss:  0.35893121361732483\n",
      "epoch:  63  loss:  0.35567396879196167\n",
      "epoch:  64  loss:  0.3524569571018219\n",
      "epoch:  65  loss:  0.3492795526981354\n",
      "epoch:  66  loss:  0.3461413085460663\n",
      "epoch:  67  loss:  0.34304165840148926\n",
      "epoch:  68  loss:  0.3399801254272461\n",
      "epoch:  69  loss:  0.336956262588501\n",
      "epoch:  70  loss:  0.33396947383880615\n",
      "epoch:  71  loss:  0.3310192823410034\n",
      "epoch:  72  loss:  0.32810527086257935\n",
      "epoch:  73  loss:  0.32522696256637573\n",
      "epoch:  74  loss:  0.3223838210105896\n",
      "epoch:  75  loss:  0.3195754587650299\n",
      "epoch:  76  loss:  0.3168013095855713\n",
      "epoch:  77  loss:  0.3140610456466675\n",
      "epoch:  78  loss:  0.3113541603088379\n",
      "epoch:  79  loss:  0.3086802363395691\n",
      "epoch:  80  loss:  0.30603882670402527\n",
      "epoch:  81  loss:  0.3034294843673706\n",
      "epoch:  82  loss:  0.30085188150405884\n",
      "epoch:  83  loss:  0.2983055114746094\n",
      "epoch:  84  loss:  0.2957899570465088\n",
      "epoch:  85  loss:  0.2933048605918884\n",
      "epoch:  86  loss:  0.29084980487823486\n",
      "epoch:  87  loss:  0.28842443227767944\n",
      "epoch:  88  loss:  0.28602829575538635\n",
      "epoch:  89  loss:  0.28366100788116455\n",
      "epoch:  90  loss:  0.28132230043411255\n",
      "epoch:  91  loss:  0.279011607170105\n",
      "epoch:  92  loss:  0.2767287790775299\n",
      "epoch:  93  loss:  0.2744733393192291\n",
      "epoch:  94  loss:  0.2722449004650116\n",
      "epoch:  95  loss:  0.27004319429397583\n",
      "epoch:  96  loss:  0.2678678035736084\n",
      "epoch:  97  loss:  0.26571840047836304\n",
      "epoch:  98  loss:  0.2635946571826935\n",
      "epoch:  99  loss:  0.26149624586105347\n",
      "epoch:  100  loss:  0.25942277908325195\n",
      "epoch:  101  loss:  0.25737398862838745\n",
      "epoch:  102  loss:  0.25534960627555847\n",
      "epoch:  103  loss:  0.2533492147922516\n",
      "epoch:  104  loss:  0.2513725161552429\n",
      "epoch:  105  loss:  0.24941927194595337\n",
      "epoch:  106  loss:  0.2474890947341919\n",
      "epoch:  107  loss:  0.24558168649673462\n",
      "epoch:  108  loss:  0.24369679391384125\n",
      "epoch:  109  loss:  0.2418341338634491\n",
      "epoch:  110  loss:  0.2399933934211731\n",
      "epoch:  111  loss:  0.23817425966262817\n",
      "epoch:  112  loss:  0.23637652397155762\n",
      "epoch:  113  loss:  0.23459985852241516\n",
      "epoch:  114  loss:  0.23284396529197693\n",
      "epoch:  115  loss:  0.2311086356639862\n",
      "epoch:  116  loss:  0.22939354181289673\n",
      "epoch:  117  loss:  0.22769847512245178\n",
      "epoch:  118  loss:  0.22602316737174988\n",
      "epoch:  119  loss:  0.22436738014221191\n",
      "epoch:  120  loss:  0.22273077070713043\n",
      "epoch:  121  loss:  0.2211131602525711\n",
      "epoch:  122  loss:  0.21951429545879364\n",
      "epoch:  123  loss:  0.21793389320373535\n",
      "epoch:  124  loss:  0.2163717895746231\n",
      "epoch:  125  loss:  0.2148277312517166\n",
      "epoch:  126  loss:  0.213301420211792\n",
      "epoch:  127  loss:  0.2117927074432373\n",
      "epoch:  128  loss:  0.21030128002166748\n",
      "epoch:  129  loss:  0.20882698893547058\n",
      "epoch:  130  loss:  0.2073695957660675\n",
      "epoch:  131  loss:  0.20592883229255676\n",
      "epoch:  132  loss:  0.2045045793056488\n",
      "epoch:  133  loss:  0.20309655368328094\n",
      "epoch:  134  loss:  0.20170456171035767\n",
      "epoch:  135  loss:  0.20032839477062225\n",
      "epoch:  136  loss:  0.198967844247818\n",
      "epoch:  137  loss:  0.19762274622917175\n",
      "epoch:  138  loss:  0.19629281759262085\n",
      "epoch:  139  loss:  0.19497795403003693\n",
      "epoch:  140  loss:  0.19367793202400208\n",
      "epoch:  141  loss:  0.1923925280570984\n",
      "epoch:  142  loss:  0.19112162292003632\n",
      "epoch:  143  loss:  0.18986496329307556\n",
      "epoch:  144  loss:  0.188622385263443\n",
      "epoch:  145  loss:  0.18739373981952667\n",
      "epoch:  146  loss:  0.1861788034439087\n",
      "epoch:  147  loss:  0.18497741222381592\n",
      "epoch:  148  loss:  0.1837894320487976\n",
      "epoch:  149  loss:  0.18261465430259705\n",
      "epoch:  150  loss:  0.1814529001712799\n",
      "epoch:  151  loss:  0.18030402064323425\n",
      "epoch:  152  loss:  0.17916786670684814\n",
      "epoch:  153  loss:  0.17804425954818726\n",
      "epoch:  154  loss:  0.17693299055099487\n",
      "epoch:  155  loss:  0.17583397030830383\n",
      "epoch:  156  loss:  0.1747470498085022\n",
      "epoch:  157  loss:  0.17367199063301086\n",
      "epoch:  158  loss:  0.17260876297950745\n",
      "epoch:  159  loss:  0.17155714333057404\n",
      "epoch:  160  loss:  0.1705169379711151\n",
      "epoch:  161  loss:  0.1694880872964859\n",
      "epoch:  162  loss:  0.16847042739391327\n",
      "epoch:  163  loss:  0.1674637794494629\n",
      "epoch:  164  loss:  0.16646802425384521\n",
      "epoch:  165  loss:  0.16548305749893188\n",
      "epoch:  166  loss:  0.1645086705684662\n",
      "epoch:  167  loss:  0.16354478895664215\n",
      "epoch:  168  loss:  0.16259129345417023\n",
      "epoch:  169  loss:  0.1616479754447937\n",
      "epoch:  170  loss:  0.1607147604227066\n",
      "epoch:  171  loss:  0.159791499376297\n",
      "epoch:  172  loss:  0.15887808799743652\n",
      "epoch:  173  loss:  0.15797439217567444\n",
      "epoch:  174  loss:  0.1570802927017212\n",
      "epoch:  175  loss:  0.15619565546512604\n",
      "epoch:  176  loss:  0.15532036125659943\n",
      "epoch:  177  loss:  0.1544543206691742\n",
      "epoch:  178  loss:  0.1535973846912384\n",
      "epoch:  179  loss:  0.1527494490146637\n",
      "epoch:  180  loss:  0.15191037952899933\n",
      "epoch:  181  loss:  0.15108010172843933\n",
      "epoch:  182  loss:  0.15025848150253296\n",
      "epoch:  183  loss:  0.14944542944431305\n",
      "epoch:  184  loss:  0.14864082634449005\n",
      "epoch:  185  loss:  0.14784455299377441\n",
      "epoch:  186  loss:  0.14705653488636017\n",
      "epoch:  187  loss:  0.14627663791179657\n",
      "epoch:  188  loss:  0.14550480246543884\n",
      "epoch:  189  loss:  0.14474086463451385\n",
      "epoch:  190  loss:  0.14398476481437683\n",
      "epoch:  191  loss:  0.14323639869689941\n",
      "epoch:  192  loss:  0.14249569177627563\n",
      "epoch:  193  loss:  0.14176249504089355\n",
      "epoch:  194  loss:  0.1410367786884308\n",
      "epoch:  195  loss:  0.1403183788061142\n",
      "epoch:  196  loss:  0.1396072953939438\n",
      "epoch:  197  loss:  0.13890333473682404\n",
      "epoch:  198  loss:  0.13820646703243256\n",
      "epoch:  199  loss:  0.13751661777496338\n",
      "epoch:  200  loss:  0.13683366775512695\n",
      "epoch:  201  loss:  0.13615752756595612\n",
      "epoch:  202  loss:  0.13548815250396729\n",
      "epoch:  203  loss:  0.13482540845870972\n",
      "epoch:  204  loss:  0.13416923582553864\n",
      "epoch:  205  loss:  0.13351957499980927\n",
      "epoch:  206  loss:  0.13287630677223206\n",
      "epoch:  207  loss:  0.13223937153816223\n",
      "epoch:  208  loss:  0.13160870969295502\n",
      "epoch:  209  loss:  0.13098421692848206\n",
      "epoch:  210  loss:  0.13036580383777618\n",
      "epoch:  211  loss:  0.12975344061851501\n",
      "epoch:  212  loss:  0.1291469931602478\n",
      "epoch:  213  loss:  0.12854644656181335\n",
      "epoch:  214  loss:  0.12795168161392212\n",
      "epoch:  215  loss:  0.1273626685142517\n",
      "epoch:  216  loss:  0.12677931785583496\n",
      "epoch:  217  loss:  0.1262015402317047\n",
      "epoch:  218  loss:  0.12562932074069977\n",
      "epoch:  219  loss:  0.12506252527236938\n",
      "epoch:  220  loss:  0.12450114637613297\n",
      "epoch:  221  loss:  0.12394508719444275\n",
      "epoch:  222  loss:  0.12339425832033157\n",
      "epoch:  223  loss:  0.12284865230321884\n",
      "epoch:  224  loss:  0.1223081648349762\n",
      "epoch:  225  loss:  0.12177274376153946\n",
      "epoch:  226  loss:  0.12124236673116684\n",
      "epoch:  227  loss:  0.12071689963340759\n",
      "epoch:  228  loss:  0.12019631266593933\n",
      "epoch:  229  loss:  0.11968056112527847\n",
      "epoch:  230  loss:  0.11916958540678024\n",
      "epoch:  231  loss:  0.11866331100463867\n",
      "epoch:  232  loss:  0.11816167086362839\n",
      "epoch:  233  loss:  0.11766466498374939\n",
      "epoch:  234  loss:  0.11717217415571213\n",
      "epoch:  235  loss:  0.11668416857719421\n",
      "epoch:  236  loss:  0.11620061844587326\n",
      "epoch:  237  loss:  0.1157214492559433\n",
      "epoch:  238  loss:  0.11524656414985657\n",
      "epoch:  239  loss:  0.11477597802877426\n",
      "epoch:  240  loss:  0.1143096312880516\n",
      "epoch:  241  loss:  0.11384742707014084\n",
      "epoch:  242  loss:  0.11338934302330017\n",
      "epoch:  243  loss:  0.11293534189462662\n",
      "epoch:  244  loss:  0.112485371530056\n",
      "epoch:  245  loss:  0.11203934252262115\n",
      "epoch:  246  loss:  0.11159727722406387\n",
      "epoch:  247  loss:  0.11115908622741699\n",
      "epoch:  248  loss:  0.11072471737861633\n",
      "epoch:  249  loss:  0.1102941483259201\n",
      "epoch:  250  loss:  0.10986731946468353\n",
      "epoch:  251  loss:  0.10944418609142303\n",
      "epoch:  252  loss:  0.10902470350265503\n",
      "epoch:  253  loss:  0.10860884189605713\n",
      "epoch:  254  loss:  0.10819655656814575\n",
      "epoch:  255  loss:  0.10778777301311493\n",
      "epoch:  256  loss:  0.10738250613212585\n",
      "epoch:  257  loss:  0.10698063671588898\n",
      "epoch:  258  loss:  0.10658220946788788\n",
      "epoch:  259  loss:  0.1061871349811554\n",
      "epoch:  260  loss:  0.10579536855220795\n",
      "epoch:  261  loss:  0.10540691018104553\n",
      "epoch:  262  loss:  0.10502167046070099\n",
      "epoch:  263  loss:  0.10463964939117432\n",
      "epoch:  264  loss:  0.10426080226898193\n",
      "epoch:  265  loss:  0.10388508439064026\n",
      "epoch:  266  loss:  0.10351245850324631\n",
      "epoch:  267  loss:  0.10314290970563889\n",
      "epoch:  268  loss:  0.10277636349201202\n",
      "epoch:  269  loss:  0.10241281986236572\n",
      "epoch:  270  loss:  0.1020522266626358\n",
      "epoch:  271  loss:  0.10169456154108047\n",
      "epoch:  272  loss:  0.10133976489305496\n",
      "epoch:  273  loss:  0.10098783671855927\n",
      "epoch:  274  loss:  0.10063871741294861\n",
      "epoch:  275  loss:  0.1002923846244812\n",
      "epoch:  276  loss:  0.09994880110025406\n",
      "epoch:  277  loss:  0.09960795938968658\n",
      "epoch:  278  loss:  0.0992698073387146\n",
      "epoch:  279  loss:  0.09893429279327393\n",
      "epoch:  280  loss:  0.09860142320394516\n",
      "epoch:  281  loss:  0.09827112406492233\n",
      "epoch:  282  loss:  0.09794342517852783\n",
      "epoch:  283  loss:  0.09761826694011688\n",
      "epoch:  284  loss:  0.09729562699794769\n",
      "epoch:  285  loss:  0.09697543829679489\n",
      "epoch:  286  loss:  0.09665773808956146\n",
      "epoch:  287  loss:  0.09634244441986084\n",
      "epoch:  288  loss:  0.09602956473827362\n",
      "epoch:  289  loss:  0.09571903944015503\n",
      "epoch:  290  loss:  0.09541086107492447\n",
      "epoch:  291  loss:  0.09510500729084015\n",
      "epoch:  292  loss:  0.09480142593383789\n",
      "epoch:  293  loss:  0.09450012445449829\n",
      "epoch:  294  loss:  0.09420107305049896\n",
      "epoch:  295  loss:  0.09390422701835632\n",
      "epoch:  296  loss:  0.09360956400632858\n",
      "epoch:  297  loss:  0.09331707656383514\n",
      "epoch:  298  loss:  0.09302672743797302\n",
      "epoch:  299  loss:  0.09273850172758102\n",
      "epoch:  300  loss:  0.09245237708091736\n",
      "epoch:  301  loss:  0.09216829389333725\n",
      "epoch:  302  loss:  0.09188628196716309\n",
      "epoch:  303  loss:  0.09160628914833069\n",
      "epoch:  304  loss:  0.09132829308509827\n",
      "epoch:  305  loss:  0.09105227142572403\n",
      "epoch:  306  loss:  0.09077823162078857\n",
      "epoch:  307  loss:  0.09050609171390533\n",
      "epoch:  308  loss:  0.09023590385913849\n",
      "epoch:  309  loss:  0.08996759355068207\n",
      "epoch:  310  loss:  0.08970116078853607\n",
      "epoch:  311  loss:  0.08943656086921692\n",
      "epoch:  312  loss:  0.0891738012433052\n",
      "epoch:  313  loss:  0.08891287446022034\n",
      "epoch:  314  loss:  0.08865372091531754\n",
      "epoch:  315  loss:  0.0883963406085968\n",
      "epoch:  316  loss:  0.08814072608947754\n",
      "epoch:  317  loss:  0.08788682520389557\n",
      "epoch:  318  loss:  0.08763465285301208\n",
      "epoch:  319  loss:  0.0873841643333435\n",
      "epoch:  320  loss:  0.08713535964488983\n",
      "epoch:  321  loss:  0.08688825368881226\n",
      "epoch:  322  loss:  0.08664273470640182\n",
      "epoch:  323  loss:  0.08639886230230331\n",
      "epoch:  324  loss:  0.08615659922361374\n",
      "epoch:  325  loss:  0.08591592311859131\n",
      "epoch:  326  loss:  0.08567682653665543\n",
      "epoch:  327  loss:  0.0854392722249031\n",
      "epoch:  328  loss:  0.08520327508449554\n",
      "epoch:  329  loss:  0.08496878296136856\n",
      "epoch:  330  loss:  0.08473581820726395\n",
      "epoch:  331  loss:  0.08450432866811752\n",
      "epoch:  332  loss:  0.08427432179450989\n",
      "epoch:  333  loss:  0.08404576778411865\n",
      "epoch:  334  loss:  0.08381866663694382\n",
      "epoch:  335  loss:  0.08359299600124359\n",
      "epoch:  336  loss:  0.08336874097585678\n",
      "epoch:  337  loss:  0.0831458568572998\n",
      "epoch:  338  loss:  0.08292437344789505\n",
      "epoch:  339  loss:  0.08270426094532013\n",
      "epoch:  340  loss:  0.08248553425073624\n",
      "epoch:  341  loss:  0.082268126308918\n",
      "epoch:  342  loss:  0.08205203711986542\n",
      "epoch:  343  loss:  0.0818372592329979\n",
      "epoch:  344  loss:  0.08162379264831543\n",
      "epoch:  345  loss:  0.08141161501407623\n",
      "epoch:  346  loss:  0.08120070397853851\n",
      "epoch:  347  loss:  0.08099106699228287\n",
      "epoch:  348  loss:  0.0807826817035675\n",
      "epoch:  349  loss:  0.08057551085948944\n",
      "epoch:  350  loss:  0.08036957681179047\n",
      "epoch:  351  loss:  0.0801648497581482\n",
      "epoch:  352  loss:  0.07996129989624023\n",
      "epoch:  353  loss:  0.07975896447896957\n",
      "epoch:  354  loss:  0.07955779880285263\n",
      "epoch:  355  loss:  0.07935778051614761\n",
      "epoch:  356  loss:  0.07915893197059631\n",
      "epoch:  357  loss:  0.07896119356155396\n",
      "epoch:  358  loss:  0.07876460254192352\n",
      "epoch:  359  loss:  0.07856912165880203\n",
      "epoch:  360  loss:  0.07837475091218948\n",
      "epoch:  361  loss:  0.07818146049976349\n",
      "epoch:  362  loss:  0.07798928022384644\n",
      "epoch:  363  loss:  0.07779814302921295\n",
      "epoch:  364  loss:  0.07760810106992722\n",
      "epoch:  365  loss:  0.07741907238960266\n",
      "epoch:  366  loss:  0.07723110914230347\n",
      "epoch:  367  loss:  0.07704417407512665\n",
      "epoch:  368  loss:  0.07685823738574982\n",
      "epoch:  369  loss:  0.07667333632707596\n",
      "epoch:  370  loss:  0.0764894187450409\n",
      "epoch:  371  loss:  0.07630650699138641\n",
      "epoch:  372  loss:  0.07612456381320953\n",
      "epoch:  373  loss:  0.07594358921051025\n",
      "epoch:  374  loss:  0.07576358318328857\n",
      "epoch:  375  loss:  0.0755845457315445\n",
      "epoch:  376  loss:  0.07540643215179443\n",
      "epoch:  377  loss:  0.07522927224636078\n",
      "epoch:  378  loss:  0.07505302131175995\n",
      "epoch:  379  loss:  0.07487768679857254\n",
      "epoch:  380  loss:  0.07470325380563736\n",
      "epoch:  381  loss:  0.0745297372341156\n",
      "epoch:  382  loss:  0.07435711473226547\n",
      "epoch:  383  loss:  0.07418537139892578\n",
      "epoch:  384  loss:  0.07401447743177414\n",
      "epoch:  385  loss:  0.07384447753429413\n",
      "epoch:  386  loss:  0.07367533445358276\n",
      "epoch:  387  loss:  0.07350703328847885\n",
      "epoch:  388  loss:  0.0733395591378212\n",
      "epoch:  389  loss:  0.07317294180393219\n",
      "epoch:  390  loss:  0.07300714403390884\n",
      "epoch:  391  loss:  0.07284215092658997\n",
      "epoch:  392  loss:  0.07267797738313675\n",
      "epoch:  393  loss:  0.0725146234035492\n",
      "epoch:  394  loss:  0.07235205173492432\n",
      "epoch:  395  loss:  0.0721902847290039\n",
      "epoch:  396  loss:  0.07202927768230438\n",
      "epoch:  397  loss:  0.07186906039714813\n",
      "epoch:  398  loss:  0.07170960307121277\n",
      "epoch:  399  loss:  0.07155091315507889\n",
      "epoch:  400  loss:  0.0713929682970047\n",
      "epoch:  401  loss:  0.0712357759475708\n",
      "epoch:  402  loss:  0.071079321205616\n",
      "epoch:  403  loss:  0.07092361152172089\n",
      "epoch:  404  loss:  0.07076861709356308\n",
      "epoch:  405  loss:  0.07061434537172318\n",
      "epoch:  406  loss:  0.07046078890562057\n",
      "epoch:  407  loss:  0.07030794024467468\n",
      "epoch:  408  loss:  0.0701557844877243\n",
      "epoch:  409  loss:  0.07000435143709183\n",
      "epoch:  410  loss:  0.06985357403755188\n",
      "epoch:  411  loss:  0.06970350444316864\n",
      "epoch:  412  loss:  0.06955409795045853\n",
      "epoch:  413  loss:  0.06940538436174393\n",
      "epoch:  414  loss:  0.06925731152296066\n",
      "epoch:  415  loss:  0.06910990923643112\n",
      "epoch:  416  loss:  0.06896315515041351\n",
      "epoch:  417  loss:  0.06881705671548843\n",
      "epoch:  418  loss:  0.06867159903049469\n",
      "epoch:  419  loss:  0.06852678209543228\n",
      "epoch:  420  loss:  0.06838259100914001\n",
      "epoch:  421  loss:  0.06823902577161789\n",
      "epoch:  422  loss:  0.0680961012840271\n",
      "epoch:  423  loss:  0.06795377284288406\n",
      "epoch:  424  loss:  0.06781205534934998\n",
      "epoch:  425  loss:  0.06767097115516663\n",
      "epoch:  426  loss:  0.06753045320510864\n",
      "epoch:  427  loss:  0.06739053130149841\n",
      "epoch:  428  loss:  0.06725123524665833\n",
      "epoch:  429  loss:  0.067112497985363\n",
      "epoch:  430  loss:  0.06697434186935425\n",
      "epoch:  431  loss:  0.06683676689863205\n",
      "epoch:  432  loss:  0.06669978052377701\n",
      "epoch:  433  loss:  0.06656333804130554\n",
      "epoch:  434  loss:  0.06642746925354004\n",
      "epoch:  435  loss:  0.06629215180873871\n",
      "epoch:  436  loss:  0.06615738570690155\n",
      "epoch:  437  loss:  0.06602317094802856\n",
      "epoch:  438  loss:  0.06588950008153915\n",
      "epoch:  439  loss:  0.06575638055801392\n",
      "epoch:  440  loss:  0.06562377512454987\n",
      "epoch:  441  loss:  0.06549171358346939\n",
      "epoch:  442  loss:  0.06536019593477249\n",
      "epoch:  443  loss:  0.06522917747497559\n",
      "epoch:  444  loss:  0.06509868800640106\n",
      "epoch:  445  loss:  0.06496872007846832\n",
      "epoch:  446  loss:  0.06483924388885498\n",
      "epoch:  447  loss:  0.06471029669046402\n",
      "epoch:  448  loss:  0.06458182632923126\n",
      "epoch:  449  loss:  0.06445387005805969\n",
      "epoch:  450  loss:  0.06432640552520752\n",
      "epoch:  451  loss:  0.06419943273067474\n",
      "epoch:  452  loss:  0.06407295167446136\n",
      "epoch:  453  loss:  0.06394694745540619\n",
      "epoch:  454  loss:  0.06382144242525101\n",
      "epoch:  455  loss:  0.06369638442993164\n",
      "epoch:  456  loss:  0.06357182562351227\n",
      "epoch:  457  loss:  0.06344771385192871\n",
      "epoch:  458  loss:  0.06332408636808395\n",
      "epoch:  459  loss:  0.0632009282708168\n",
      "epoch:  460  loss:  0.06307820975780487\n",
      "epoch:  461  loss:  0.06295593827962875\n",
      "epoch:  462  loss:  0.06283415108919144\n",
      "epoch:  463  loss:  0.06271278113126755\n",
      "epoch:  464  loss:  0.06259186565876007\n",
      "epoch:  465  loss:  0.062471386045217514\n",
      "epoch:  466  loss:  0.06235136464238167\n",
      "epoch:  467  loss:  0.062231771647930145\n",
      "epoch:  468  loss:  0.06211259961128235\n",
      "epoch:  469  loss:  0.061993859708309174\n",
      "epoch:  470  loss:  0.06187555938959122\n",
      "epoch:  471  loss:  0.061757683753967285\n",
      "epoch:  472  loss:  0.061640217900276184\n",
      "epoch:  473  loss:  0.06152316927909851\n",
      "epoch:  474  loss:  0.061406515538692474\n",
      "epoch:  475  loss:  0.061290305107831955\n",
      "epoch:  476  loss:  0.06117449328303337\n",
      "epoch:  477  loss:  0.06105908751487732\n",
      "epoch:  478  loss:  0.0609440803527832\n",
      "epoch:  479  loss:  0.06082947552204132\n",
      "epoch:  480  loss:  0.060715265572071075\n",
      "epoch:  481  loss:  0.06060144305229187\n",
      "epoch:  482  loss:  0.0604880154132843\n",
      "epoch:  483  loss:  0.06037497892975807\n",
      "epoch:  484  loss:  0.06026234105229378\n",
      "epoch:  485  loss:  0.06015006825327873\n",
      "epoch:  486  loss:  0.06003819406032562\n",
      "epoch:  487  loss:  0.059926681220531464\n",
      "epoch:  488  loss:  0.05981553718447685\n",
      "epoch:  489  loss:  0.05970480665564537\n",
      "epoch:  490  loss:  0.05959440767765045\n",
      "epoch:  491  loss:  0.05948439985513687\n",
      "epoch:  492  loss:  0.05937473848462105\n",
      "epoch:  493  loss:  0.059265442192554474\n",
      "epoch:  494  loss:  0.05915652588009834\n",
      "epoch:  495  loss:  0.05904795601963997\n",
      "epoch:  496  loss:  0.05893974378705025\n",
      "epoch:  497  loss:  0.05883188545703888\n",
      "epoch:  498  loss:  0.05872437730431557\n",
      "epoch:  499  loss:  0.05861721187829971\n",
      "epoch:  500  loss:  0.05851040035486221\n",
      "epoch:  501  loss:  0.05840392783284187\n",
      "epoch:  502  loss:  0.058297812938690186\n",
      "epoch:  503  loss:  0.058192018419504166\n",
      "epoch:  504  loss:  0.0580865740776062\n",
      "epoch:  505  loss:  0.0579814687371254\n",
      "epoch:  506  loss:  0.05787668377161026\n",
      "epoch:  507  loss:  0.057772234082221985\n",
      "epoch:  508  loss:  0.057668134570121765\n",
      "epoch:  509  loss:  0.05756434053182602\n",
      "epoch:  510  loss:  0.057460881769657135\n",
      "epoch:  511  loss:  0.05735773965716362\n",
      "epoch:  512  loss:  0.057254936546087265\n",
      "epoch:  513  loss:  0.057152435183525085\n",
      "epoch:  514  loss:  0.05705024302005768\n",
      "epoch:  515  loss:  0.05694838985800743\n",
      "epoch:  516  loss:  0.05684685334563255\n",
      "epoch:  517  loss:  0.056745611131191254\n",
      "epoch:  518  loss:  0.056644685566425323\n",
      "epoch:  519  loss:  0.056544072926044464\n",
      "epoch:  520  loss:  0.05644374340772629\n",
      "epoch:  521  loss:  0.056343741714954376\n",
      "epoch:  522  loss:  0.05624403804540634\n",
      "epoch:  523  loss:  0.056144632399082184\n",
      "epoch:  524  loss:  0.0560455322265625\n",
      "epoch:  525  loss:  0.055946722626686096\n",
      "epoch:  526  loss:  0.055848218500614166\n",
      "epoch:  527  loss:  0.05574999377131462\n",
      "epoch:  528  loss:  0.05565207824110985\n",
      "epoch:  529  loss:  0.055554427206516266\n",
      "epoch:  530  loss:  0.055457085371017456\n",
      "epoch:  531  loss:  0.05536000803112984\n",
      "epoch:  532  loss:  0.05526324361562729\n",
      "epoch:  533  loss:  0.05516676604747772\n",
      "epoch:  534  loss:  0.05507054179906845\n",
      "epoch:  535  loss:  0.05497461557388306\n",
      "epoch:  536  loss:  0.054878972470760345\n",
      "epoch:  537  loss:  0.05478360503911972\n",
      "epoch:  538  loss:  0.05468851327896118\n",
      "epoch:  539  loss:  0.054593686014413834\n",
      "epoch:  540  loss:  0.05449913442134857\n",
      "epoch:  541  loss:  0.05440486595034599\n",
      "epoch:  542  loss:  0.054310865700244904\n",
      "epoch:  543  loss:  0.05421711504459381\n",
      "epoch:  544  loss:  0.0541236475110054\n",
      "epoch:  545  loss:  0.05403043329715729\n",
      "epoch:  546  loss:  0.053937505930662155\n",
      "epoch:  547  loss:  0.05384483188390732\n",
      "epoch:  548  loss:  0.05375239998102188\n",
      "epoch:  549  loss:  0.05366024002432823\n",
      "epoch:  550  loss:  0.053568340837955475\n",
      "epoch:  551  loss:  0.05347669497132301\n",
      "epoch:  552  loss:  0.05338531732559204\n",
      "epoch:  553  loss:  0.05329418182373047\n",
      "epoch:  554  loss:  0.053203314542770386\n",
      "epoch:  555  loss:  0.05311267822980881\n",
      "epoch:  556  loss:  0.05302230268716812\n",
      "epoch:  557  loss:  0.05293216556310654\n",
      "epoch:  558  loss:  0.052842285484075546\n",
      "epoch:  559  loss:  0.05275264382362366\n",
      "epoch:  560  loss:  0.05266326665878296\n",
      "epoch:  561  loss:  0.05257410556077957\n",
      "epoch:  562  loss:  0.05248519778251648\n",
      "epoch:  563  loss:  0.05239655077457428\n",
      "epoch:  564  loss:  0.05230811983346939\n",
      "epoch:  565  loss:  0.0522199347615242\n",
      "epoch:  566  loss:  0.05213199183344841\n",
      "epoch:  567  loss:  0.052044279873371124\n",
      "epoch:  568  loss:  0.05195681005716324\n",
      "epoch:  569  loss:  0.051869578659534454\n",
      "epoch:  570  loss:  0.051782555878162384\n",
      "epoch:  571  loss:  0.05169578641653061\n",
      "epoch:  572  loss:  0.05160924792289734\n",
      "epoch:  573  loss:  0.051522936671972275\n",
      "epoch:  574  loss:  0.05143684893846512\n",
      "epoch:  575  loss:  0.051350973546504974\n",
      "epoch:  576  loss:  0.051265355199575424\n",
      "epoch:  577  loss:  0.05117993801832199\n",
      "epoch:  578  loss:  0.051094748079776764\n",
      "epoch:  579  loss:  0.051009807735681534\n",
      "epoch:  580  loss:  0.050925061106681824\n",
      "epoch:  581  loss:  0.05084053799510002\n",
      "epoch:  582  loss:  0.050756245851516724\n",
      "epoch:  583  loss:  0.05067216977477074\n",
      "epoch:  584  loss:  0.050588302314281464\n",
      "epoch:  585  loss:  0.0505046546459198\n",
      "epoch:  586  loss:  0.050421226769685745\n",
      "epoch:  587  loss:  0.0503380186855793\n",
      "epoch:  588  loss:  0.05025502294301987\n",
      "epoch:  589  loss:  0.05017223209142685\n",
      "epoch:  590  loss:  0.050089649856090546\n",
      "epoch:  591  loss:  0.05000729113817215\n",
      "epoch:  592  loss:  0.049925122410058975\n",
      "epoch:  593  loss:  0.049843184649944305\n",
      "epoch:  594  loss:  0.04976145178079605\n",
      "epoch:  595  loss:  0.049679916352033615\n",
      "epoch:  596  loss:  0.04959859699010849\n",
      "epoch:  597  loss:  0.04951747506856918\n",
      "epoch:  598  loss:  0.04943656921386719\n",
      "epoch:  599  loss:  0.049355845898389816\n",
      "epoch:  600  loss:  0.04927533119916916\n",
      "epoch:  601  loss:  0.049195028841495514\n",
      "epoch:  602  loss:  0.04911491274833679\n",
      "epoch:  603  loss:  0.049035001546144485\n",
      "epoch:  604  loss:  0.048955291509628296\n",
      "epoch:  605  loss:  0.04887578636407852\n",
      "epoch:  606  loss:  0.04879646748304367\n",
      "epoch:  607  loss:  0.04871734231710434\n",
      "epoch:  608  loss:  0.04863841086626053\n",
      "epoch:  609  loss:  0.04855969920754433\n",
      "epoch:  610  loss:  0.04848115146160126\n",
      "epoch:  611  loss:  0.0484028086066246\n",
      "epoch:  612  loss:  0.04832465574145317\n",
      "epoch:  613  loss:  0.04824669659137726\n",
      "epoch:  614  loss:  0.048168931156396866\n",
      "epoch:  615  loss:  0.0480913408100605\n",
      "epoch:  616  loss:  0.04801395162940025\n",
      "epoch:  617  loss:  0.04793674871325493\n",
      "epoch:  618  loss:  0.04785973206162453\n",
      "epoch:  619  loss:  0.04778289794921875\n",
      "epoch:  620  loss:  0.04770626127719879\n",
      "epoch:  621  loss:  0.04762980341911316\n",
      "epoch:  622  loss:  0.047553516924381256\n",
      "epoch:  623  loss:  0.04747742787003517\n",
      "epoch:  624  loss:  0.04740152508020401\n",
      "epoch:  625  loss:  0.047325797379016876\n",
      "epoch:  626  loss:  0.04725024476647377\n",
      "epoch:  627  loss:  0.047174882143735886\n",
      "epoch:  628  loss:  0.04709968715906143\n",
      "epoch:  629  loss:  0.0470246858894825\n",
      "epoch:  630  loss:  0.046949852257966995\n",
      "epoch:  631  loss:  0.046875208616256714\n",
      "epoch:  632  loss:  0.046800725162029266\n",
      "epoch:  633  loss:  0.04672641307115555\n",
      "epoch:  634  loss:  0.04665229097008705\n",
      "epoch:  635  loss:  0.04657835140824318\n",
      "epoch:  636  loss:  0.046504564583301544\n",
      "epoch:  637  loss:  0.04643097147345543\n",
      "epoch:  638  loss:  0.046357542276382446\n",
      "epoch:  639  loss:  0.046284280717372894\n",
      "epoch:  640  loss:  0.046211183071136475\n",
      "epoch:  641  loss:  0.04613826423883438\n",
      "epoch:  642  loss:  0.04606551676988602\n",
      "epoch:  643  loss:  0.04599294438958168\n",
      "epoch:  644  loss:  0.045920535922050476\n",
      "epoch:  645  loss:  0.04584828019142151\n",
      "epoch:  646  loss:  0.045776210725307465\n",
      "epoch:  647  loss:  0.04570431262254715\n",
      "epoch:  648  loss:  0.04563256353139877\n",
      "epoch:  649  loss:  0.04556099325418472\n",
      "epoch:  650  loss:  0.04548957198858261\n",
      "epoch:  651  loss:  0.045418329536914825\n",
      "epoch:  652  loss:  0.045347243547439575\n",
      "epoch:  653  loss:  0.045276325196027756\n",
      "epoch:  654  loss:  0.04520556703209877\n",
      "epoch:  655  loss:  0.04513496160507202\n",
      "epoch:  656  loss:  0.045064527541399\n",
      "epoch:  657  loss:  0.044994253665208817\n",
      "epoch:  658  loss:  0.04492414370179176\n",
      "epoch:  659  loss:  0.044854190200567245\n",
      "epoch:  660  loss:  0.044784389436244965\n",
      "epoch:  661  loss:  0.04471474140882492\n",
      "epoch:  662  loss:  0.04464525729417801\n",
      "epoch:  663  loss:  0.04457593709230423\n",
      "epoch:  664  loss:  0.04450676217675209\n",
      "epoch:  665  loss:  0.044437751173973083\n",
      "epoch:  666  loss:  0.04436888545751572\n",
      "epoch:  667  loss:  0.04430018365383148\n",
      "epoch:  668  loss:  0.044231630861759186\n",
      "epoch:  669  loss:  0.04416324943304062\n",
      "epoch:  670  loss:  0.0440949983894825\n",
      "epoch:  671  loss:  0.04402691125869751\n",
      "epoch:  672  loss:  0.04395897686481476\n",
      "epoch:  673  loss:  0.04389118775725365\n",
      "epoch:  674  loss:  0.043823547661304474\n",
      "epoch:  675  loss:  0.04375607520341873\n",
      "epoch:  676  loss:  0.04368872195482254\n",
      "epoch:  677  loss:  0.043621547520160675\n",
      "epoch:  678  loss:  0.043554507195949554\n",
      "epoch:  679  loss:  0.043487608432769775\n",
      "epoch:  680  loss:  0.04342088848352432\n",
      "epoch:  681  loss:  0.043354276567697525\n",
      "epoch:  682  loss:  0.04328783601522446\n",
      "epoch:  683  loss:  0.04322154074907303\n",
      "epoch:  684  loss:  0.04315538704395294\n",
      "epoch:  685  loss:  0.043089356273412704\n",
      "epoch:  686  loss:  0.04302350431680679\n",
      "epoch:  687  loss:  0.04295777156949043\n",
      "epoch:  688  loss:  0.04289218783378601\n",
      "epoch:  689  loss:  0.04282676428556442\n",
      "epoch:  690  loss:  0.04276146739721298\n",
      "epoch:  691  loss:  0.04269631952047348\n",
      "epoch:  692  loss:  0.04263129457831383\n",
      "epoch:  693  loss:  0.04256643354892731\n",
      "epoch:  694  loss:  0.04250170290470123\n",
      "epoch:  695  loss:  0.0424371175467968\n",
      "epoch:  696  loss:  0.042372673749923706\n",
      "epoch:  697  loss:  0.04230837523937225\n",
      "epoch:  698  loss:  0.04224420338869095\n",
      "epoch:  699  loss:  0.042180173099040985\n",
      "epoch:  700  loss:  0.04211629182100296\n",
      "epoch:  701  loss:  0.04205253720283508\n",
      "epoch:  702  loss:  0.041988909244537354\n",
      "epoch:  703  loss:  0.04192543774843216\n",
      "epoch:  704  loss:  0.04186209291219711\n",
      "epoch:  705  loss:  0.04179888963699341\n",
      "epoch:  706  loss:  0.041735805571079254\n",
      "epoch:  707  loss:  0.04167287051677704\n",
      "epoch:  708  loss:  0.04161006212234497\n",
      "epoch:  709  loss:  0.041547395288944244\n",
      "epoch:  710  loss:  0.041484858840703964\n",
      "epoch:  711  loss:  0.04142244905233383\n",
      "epoch:  712  loss:  0.041360192000865936\n",
      "epoch:  713  loss:  0.04129806160926819\n",
      "epoch:  714  loss:  0.04123605042695999\n",
      "epoch:  715  loss:  0.04117417335510254\n",
      "epoch:  716  loss:  0.04111242666840553\n",
      "epoch:  717  loss:  0.04105081409215927\n",
      "epoch:  718  loss:  0.04098932817578316\n",
      "epoch:  719  loss:  0.04092797636985779\n",
      "epoch:  720  loss:  0.04086676239967346\n",
      "epoch:  721  loss:  0.04080565646290779\n",
      "epoch:  722  loss:  0.04074469953775406\n",
      "epoch:  723  loss:  0.040683865547180176\n",
      "epoch:  724  loss:  0.04062315449118614\n",
      "epoch:  725  loss:  0.04056256264448166\n",
      "epoch:  726  loss:  0.04050210863351822\n",
      "epoch:  727  loss:  0.04044177010655403\n",
      "epoch:  728  loss:  0.040381573140621185\n",
      "epoch:  729  loss:  0.04032149165868759\n",
      "epoch:  730  loss:  0.04026154428720474\n",
      "epoch:  731  loss:  0.040201712399721146\n",
      "epoch:  732  loss:  0.040142010897397995\n",
      "epoch:  733  loss:  0.04008243978023529\n",
      "epoch:  734  loss:  0.04002297669649124\n",
      "epoch:  735  loss:  0.03996364772319794\n",
      "epoch:  736  loss:  0.03990444913506508\n",
      "epoch:  737  loss:  0.03984535485506058\n",
      "epoch:  738  loss:  0.03978639468550682\n",
      "epoch:  739  loss:  0.03972755745053291\n",
      "epoch:  740  loss:  0.03966883569955826\n",
      "epoch:  741  loss:  0.039610233157873154\n",
      "epoch:  742  loss:  0.039551761001348495\n",
      "epoch:  743  loss:  0.03949340432882309\n",
      "epoch:  744  loss:  0.039435163140296936\n",
      "epoch:  745  loss:  0.03937704861164093\n",
      "epoch:  746  loss:  0.039319053292274475\n",
      "epoch:  747  loss:  0.039261169731616974\n",
      "epoch:  748  loss:  0.03920340538024902\n",
      "epoch:  749  loss:  0.03914577513933182\n",
      "epoch:  750  loss:  0.03908824920654297\n",
      "epoch:  751  loss:  0.03903084620833397\n",
      "epoch:  752  loss:  0.03897355496883392\n",
      "epoch:  753  loss:  0.03891638666391373\n",
      "epoch:  754  loss:  0.03885932266712189\n",
      "epoch:  755  loss:  0.038802389055490494\n",
      "epoch:  756  loss:  0.038745563477277756\n",
      "epoch:  757  loss:  0.03868885338306427\n",
      "epoch:  758  loss:  0.03863226994872093\n",
      "epoch:  759  loss:  0.03857579454779625\n",
      "epoch:  760  loss:  0.03851943090558052\n",
      "epoch:  761  loss:  0.038463182747364044\n",
      "epoch:  762  loss:  0.03840705752372742\n",
      "epoch:  763  loss:  0.03835102543234825\n",
      "epoch:  764  loss:  0.03829512000083923\n",
      "epoch:  765  loss:  0.03823932260274887\n",
      "epoch:  766  loss:  0.03818364441394806\n",
      "epoch:  767  loss:  0.0381280779838562\n",
      "epoch:  768  loss:  0.0380726158618927\n",
      "epoch:  769  loss:  0.03801726549863815\n",
      "epoch:  770  loss:  0.037962041795253754\n",
      "epoch:  771  loss:  0.03790690004825592\n",
      "epoch:  772  loss:  0.03785190358757973\n",
      "epoch:  773  loss:  0.0377969928085804\n",
      "epoch:  774  loss:  0.037742212414741516\n",
      "epoch:  775  loss:  0.0376875214278698\n",
      "epoch:  776  loss:  0.03763296455144882\n",
      "epoch:  777  loss:  0.037578485906124115\n",
      "epoch:  778  loss:  0.03752413019537926\n",
      "epoch:  779  loss:  0.037469882518053055\n",
      "epoch:  780  loss:  0.037415750324726105\n",
      "epoch:  781  loss:  0.03736171871423721\n",
      "epoch:  782  loss:  0.03730779513716698\n",
      "epoch:  783  loss:  0.0372539721429348\n",
      "epoch:  784  loss:  0.03720026835799217\n",
      "epoch:  785  loss:  0.0371466688811779\n",
      "epoch:  786  loss:  0.03709316626191139\n",
      "epoch:  787  loss:  0.037039779126644135\n",
      "epoch:  788  loss:  0.03698650747537613\n",
      "epoch:  789  loss:  0.03693331778049469\n",
      "epoch:  790  loss:  0.0368802472949028\n",
      "epoch:  791  loss:  0.03682728856801987\n",
      "epoch:  792  loss:  0.03677442669868469\n",
      "epoch:  793  loss:  0.03672165796160698\n",
      "epoch:  794  loss:  0.03666899353265762\n",
      "epoch:  795  loss:  0.036616452038288116\n",
      "epoch:  796  loss:  0.03656400367617607\n",
      "epoch:  797  loss:  0.03651165962219238\n",
      "epoch:  798  loss:  0.03645941987633705\n",
      "epoch:  799  loss:  0.03640728071331978\n",
      "epoch:  800  loss:  0.03635523468255997\n",
      "epoch:  801  loss:  0.036303307861089706\n",
      "epoch:  802  loss:  0.03625147044658661\n",
      "epoch:  803  loss:  0.03619973734021187\n",
      "epoch:  804  loss:  0.03614811599254608\n",
      "epoch:  805  loss:  0.03609658405184746\n",
      "epoch:  806  loss:  0.03604515641927719\n",
      "epoch:  807  loss:  0.03599382936954498\n",
      "epoch:  808  loss:  0.03594260290265083\n",
      "epoch:  809  loss:  0.03589148074388504\n",
      "epoch:  810  loss:  0.03584044426679611\n",
      "epoch:  811  loss:  0.03578951209783554\n",
      "epoch:  812  loss:  0.035738684237003326\n",
      "epoch:  813  loss:  0.035687949508428574\n",
      "epoch:  814  loss:  0.03563731908798218\n",
      "epoch:  815  loss:  0.03558678179979324\n",
      "epoch:  816  loss:  0.035536352545022964\n",
      "epoch:  817  loss:  0.03548600524663925\n",
      "epoch:  818  loss:  0.03543576970696449\n",
      "epoch:  819  loss:  0.035385631024837494\n",
      "epoch:  820  loss:  0.03533557802438736\n",
      "epoch:  821  loss:  0.03528563305735588\n",
      "epoch:  822  loss:  0.03523576259613037\n",
      "epoch:  823  loss:  0.03518601879477501\n",
      "epoch:  824  loss:  0.03513635694980621\n",
      "epoch:  825  loss:  0.03508678823709488\n",
      "epoch:  826  loss:  0.035037312656641006\n",
      "epoch:  827  loss:  0.034987956285476685\n",
      "epoch:  828  loss:  0.03493865951895714\n",
      "epoch:  829  loss:  0.03488948196172714\n",
      "epoch:  830  loss:  0.03484039008617401\n",
      "epoch:  831  loss:  0.03479138761758804\n",
      "epoch:  832  loss:  0.03474249690771103\n",
      "epoch:  833  loss:  0.034693680703639984\n",
      "epoch:  834  loss:  0.034644965082407\n",
      "epoch:  835  loss:  0.03459634631872177\n",
      "epoch:  836  loss:  0.03454780951142311\n",
      "epoch:  837  loss:  0.034499384462833405\n",
      "epoch:  838  loss:  0.034451037645339966\n",
      "epoch:  839  loss:  0.03440277650952339\n",
      "epoch:  840  loss:  0.03435463085770607\n",
      "epoch:  841  loss:  0.03430655971169472\n",
      "epoch:  842  loss:  0.03425858914852142\n",
      "epoch:  843  loss:  0.0342106968164444\n",
      "epoch:  844  loss:  0.03416289761662483\n",
      "epoch:  845  loss:  0.034115198999643326\n",
      "epoch:  846  loss:  0.03406759351491928\n",
      "epoch:  847  loss:  0.034020066261291504\n",
      "epoch:  848  loss:  0.03397264704108238\n",
      "epoch:  849  loss:  0.033925291150808334\n",
      "epoch:  850  loss:  0.033878032118082047\n",
      "epoch:  851  loss:  0.03383087366819382\n",
      "epoch:  852  loss:  0.03378380462527275\n",
      "epoch:  853  loss:  0.03373682498931885\n",
      "epoch:  854  loss:  0.03368993103504181\n",
      "epoch:  855  loss:  0.03364311903715134\n",
      "epoch:  856  loss:  0.033596400171518326\n",
      "epoch:  857  loss:  0.03354978188872337\n",
      "epoch:  858  loss:  0.03350323066115379\n",
      "epoch:  859  loss:  0.03345678001642227\n",
      "epoch:  860  loss:  0.033410415053367615\n",
      "epoch:  861  loss:  0.03336413949728012\n",
      "epoch:  862  loss:  0.0333179347217083\n",
      "epoch:  863  loss:  0.03327183425426483\n",
      "epoch:  864  loss:  0.03322581201791763\n",
      "epoch:  865  loss:  0.033179882913827896\n",
      "epoch:  866  loss:  0.033134035766124725\n",
      "epoch:  867  loss:  0.03308827430009842\n",
      "epoch:  868  loss:  0.03304259479045868\n",
      "epoch:  869  loss:  0.0329970121383667\n",
      "epoch:  870  loss:  0.03295149654150009\n",
      "epoch:  871  loss:  0.03290608525276184\n",
      "epoch:  872  loss:  0.03286075219511986\n",
      "epoch:  873  loss:  0.032815493643283844\n",
      "epoch:  874  loss:  0.03277032449841499\n",
      "epoch:  875  loss:  0.0327252522110939\n",
      "epoch:  876  loss:  0.032680243253707886\n",
      "epoch:  877  loss:  0.03263534605503082\n",
      "epoch:  878  loss:  0.03259050101041794\n",
      "epoch:  879  loss:  0.03254575654864311\n",
      "epoch:  880  loss:  0.03250109404325485\n",
      "epoch:  881  loss:  0.032456524670124054\n",
      "epoch:  882  loss:  0.03241202235221863\n",
      "epoch:  883  loss:  0.032367605715990067\n",
      "epoch:  884  loss:  0.032323289662599564\n",
      "epoch:  885  loss:  0.03227902948856354\n",
      "epoch:  886  loss:  0.032234858721494675\n",
      "epoch:  887  loss:  0.032190777361392975\n",
      "epoch:  888  loss:  0.032146770507097244\n",
      "epoch:  889  loss:  0.03210284933447838\n",
      "epoch:  890  loss:  0.03205900639295578\n",
      "epoch:  891  loss:  0.03201523423194885\n",
      "epoch:  892  loss:  0.031971562653779984\n",
      "epoch:  893  loss:  0.031927961856126785\n",
      "epoch:  894  loss:  0.03188444674015045\n",
      "epoch:  895  loss:  0.03184100240468979\n",
      "epoch:  896  loss:  0.031797654926776886\n",
      "epoch:  897  loss:  0.03175437077879906\n",
      "epoch:  898  loss:  0.031711164861917496\n",
      "epoch:  899  loss:  0.0316680483520031\n",
      "epoch:  900  loss:  0.03162500634789467\n",
      "epoch:  901  loss:  0.03158204257488251\n",
      "epoch:  902  loss:  0.03153916448354721\n",
      "epoch:  903  loss:  0.031496357172727585\n",
      "epoch:  904  loss:  0.031453631818294525\n",
      "epoch:  905  loss:  0.03141098469495773\n",
      "epoch:  906  loss:  0.03136841952800751\n",
      "epoch:  907  loss:  0.03132593259215355\n",
      "epoch:  908  loss:  0.03128351271152496\n",
      "epoch:  909  loss:  0.03124118037521839\n",
      "epoch:  910  loss:  0.031198915094137192\n",
      "epoch:  911  loss:  0.03115672618150711\n",
      "epoch:  912  loss:  0.03111463412642479\n",
      "epoch:  913  loss:  0.031072599813342094\n",
      "epoch:  914  loss:  0.031030651181936264\n",
      "epoch:  915  loss:  0.0309887807816267\n",
      "epoch:  916  loss:  0.03094697743654251\n",
      "epoch:  917  loss:  0.030905259773135185\n",
      "epoch:  918  loss:  0.03086361102759838\n",
      "epoch:  919  loss:  0.0308220311999321\n",
      "epoch:  920  loss:  0.030780533328652382\n",
      "epoch:  921  loss:  0.030739115551114082\n",
      "epoch:  922  loss:  0.0306977741420269\n",
      "epoch:  923  loss:  0.030656499788165092\n",
      "epoch:  924  loss:  0.0306153055280447\n",
      "epoch:  925  loss:  0.030574191361665726\n",
      "epoch:  926  loss:  0.03053313121199608\n",
      "epoch:  927  loss:  0.03049216791987419\n",
      "epoch:  928  loss:  0.030451267957687378\n",
      "epoch:  929  loss:  0.03041045367717743\n",
      "epoch:  930  loss:  0.030369697138667107\n",
      "epoch:  931  loss:  0.030329018831253052\n",
      "epoch:  932  loss:  0.030288424342870712\n",
      "epoch:  933  loss:  0.030247889459133148\n",
      "epoch:  934  loss:  0.030207434669137\n",
      "epoch:  935  loss:  0.030167043209075928\n",
      "epoch:  936  loss:  0.03012673556804657\n",
      "epoch:  937  loss:  0.03008650243282318\n",
      "epoch:  938  loss:  0.030046332627534866\n",
      "epoch:  939  loss:  0.03000623546540737\n",
      "epoch:  940  loss:  0.029966218397021294\n",
      "epoch:  941  loss:  0.02992626093327999\n",
      "epoch:  942  loss:  0.029886391013860703\n",
      "epoch:  943  loss:  0.029846586287021637\n",
      "epoch:  944  loss:  0.029806848615407944\n",
      "epoch:  945  loss:  0.02976718544960022\n",
      "epoch:  946  loss:  0.029727593064308167\n",
      "epoch:  947  loss:  0.02968806028366089\n",
      "epoch:  948  loss:  0.029648618772625923\n",
      "epoch:  949  loss:  0.02960924245417118\n",
      "epoch:  950  loss:  0.029569923877716064\n",
      "epoch:  951  loss:  0.029530687257647514\n",
      "epoch:  952  loss:  0.029491517692804337\n",
      "epoch:  953  loss:  0.02945241704583168\n",
      "epoch:  954  loss:  0.0294133760035038\n",
      "epoch:  955  loss:  0.02937442436814308\n",
      "epoch:  956  loss:  0.02933552861213684\n",
      "epoch:  957  loss:  0.02929670922458172\n",
      "epoch:  958  loss:  0.02925795316696167\n",
      "epoch:  959  loss:  0.029219266027212143\n",
      "epoch:  960  loss:  0.029180649667978287\n",
      "epoch:  961  loss:  0.029142111539840698\n",
      "epoch:  962  loss:  0.029103627428412437\n",
      "epoch:  963  loss:  0.029065223410725594\n",
      "epoch:  964  loss:  0.029026882722973824\n",
      "epoch:  965  loss:  0.028988605365157127\n",
      "epoch:  966  loss:  0.02895040437579155\n",
      "epoch:  967  loss:  0.02891225554049015\n",
      "epoch:  968  loss:  0.02887418121099472\n",
      "epoch:  969  loss:  0.02883618138730526\n",
      "epoch:  970  loss:  0.028798246756196022\n",
      "epoch:  971  loss:  0.028760379180312157\n",
      "epoch:  972  loss:  0.028722580522298813\n",
      "epoch:  973  loss:  0.028684839606285095\n",
      "epoch:  974  loss:  0.0286471676081419\n",
      "epoch:  975  loss:  0.028609566390514374\n",
      "epoch:  976  loss:  0.028572015464305878\n",
      "epoch:  977  loss:  0.028534557670354843\n",
      "epoch:  978  loss:  0.028497155755758286\n",
      "epoch:  979  loss:  0.028459815308451653\n",
      "epoch:  980  loss:  0.028422554954886436\n",
      "epoch:  981  loss:  0.02838534489274025\n",
      "epoch:  982  loss:  0.028348198160529137\n",
      "epoch:  983  loss:  0.028311122208833694\n",
      "epoch:  984  loss:  0.028274107724428177\n",
      "epoch:  985  loss:  0.028237156569957733\n",
      "epoch:  986  loss:  0.028200287371873856\n",
      "epoch:  987  loss:  0.028163470327854156\n",
      "epoch:  988  loss:  0.02812672033905983\n",
      "epoch:  989  loss:  0.02809002622961998\n",
      "epoch:  990  loss:  0.02805340476334095\n",
      "epoch:  991  loss:  0.028016848489642143\n",
      "epoch:  992  loss:  0.02798035740852356\n",
      "epoch:  993  loss:  0.027943922206759453\n",
      "epoch:  994  loss:  0.027907555922865868\n",
      "epoch:  995  loss:  0.02787124738097191\n",
      "epoch:  996  loss:  0.02783500775694847\n",
      "epoch:  997  loss:  0.027798833325505257\n",
      "epoch:  998  loss:  0.02776271477341652\n",
      "epoch:  999  loss:  0.027726667001843452\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred = model_lr(X)\n",
    "    loss = metrics_name_1(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    Optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    Optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8caa3563-1cea-411a-94ea-164772b8b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f3fd82cc-d040-4bed-8124-a1de8a52dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_lr, loss_fn, optimizer):\n",
    "    \"\"\"function train\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    model_lr.train()\n",
    "    for batch_size_lr in range(1000):\n",
    "        y_pred_1 = model_lr(X)\n",
    "        loss = loss_fn(y_pred_1, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if BATCH_SIZE_LR % 16 == 0:\n",
    "            loss_2 = loss.item(), BATCH_SIZE_LR * len(X)\n",
    "            print(f'loss: {loss_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "478405ed-40be-4b4d-8be6-c837a171a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function train\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e437af93-5ccd-4c40-ac28-4c626765b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model_lr, loss_fn):\n",
    "    \"\"\"function test\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    num_batches = size\n",
    "    model_lr.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_size_lr in range(1000):\n",
    "            y_pred_2 = model_lr(X)\n",
    "            test_loss += loss_fn(y_pred_2, y).item()\n",
    "            test_loss /= num_batches\n",
    "            print(f'Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c2f27be2-5b82-48cc-9158-96c2939ea0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ac82c3bc-59c9-4460-b9ab-08531a707cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "663fd727-99a9-4a67-96b5-74c71daf400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed0ee0f2-ea17-4ff5-be45-306a501a2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCHS_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0eadf3ca-af2f-42e5-a683-e2f55114c239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google-chrome-stable 122 -error memory chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "86d350c7-7c05-4d65-891a-0936e766ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results(for epoch in range(EPOCHS_LR)): to see python_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b02ac295-a648-4635-95ae-b137ede36dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1678]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_lr(X_test_tensor[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1f3bbc5d-e01b-44cf-9466-5604aa33e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_tensor[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1d0c369-3143-4674-afb3-c5683dbdc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lr, \"lr.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "10069b81-ca90-445b-a50c-3c9be97307b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e16ea03a-0eab-482a-873d-688451b325ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_1 = nn.Sequential(\n",
    "    nn.Linear(in_features=n_features, out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=16, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ba9cf557-30f2-4a0d-8175-73baabe1dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch model_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b4f90a51-ad4c-4b7d-a6a1-b4f1c89ecc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=21, out_features=16, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (9): ReLU()\n",
      "  (10): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "489940af-ba73-4e06-9659-468a17d848ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch model_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1bcb3da4-ed45-4739-beb7-83b98df5a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_name_2 = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "809cc579-d689-4f16-b087-baefa26572c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0db5ecc0-d77c-4d74-8d58-a8f55976f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer = optim.SGD(params=model_lr_1.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d1ac8fe7-c8a4-408b-93d9-dcf3794ded0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.optim model_lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "0685d5b1-1574-4f33-8fd2-644e1790bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_LR_1 = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "46e7b1e4-771a-4437-9fde-60a7e65bdf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5862c34a-9c94-47d4-99e2-8fa32f64d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_LR_1 = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "65c93c9a-2ca7-4603-a6cf-8208f73982c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "413d9475-205b-4e3b-9fdc-3dc84ea9fb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [06:23<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.trange(EPOCHS_LR_1):\n",
    "    for i in range((n_data - 1) // BATCH_SIZE_LR_1 + 1):\n",
    "        start_i = i * BATCH_SIZE_LR_1\n",
    "        end_i = start_i + BATCH_SIZE_LR_1\n",
    "        Xb = X_train_tensor[start_i: end_i]\n",
    "        yb = y_train_tensor[start_i: end_i]\n",
    "        pred_2 = yb\n",
    "        loss = loss_func(pred_2, yb + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8dcd75d1-837a-40f9-803b-7d2b954a4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm.trange(EPOCHS_LR_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0307b348-b43f-4403-bda0-6c169e0825a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.14267765  0.039095    0.79884574 ... -0.98421443 -0.01879476\n",
      "  -0.09096936]\n",
      " [ 0.64735879 -0.48534494 -0.66910215 ...  0.57634728 -0.20382646\n",
      "  -0.04115568]\n",
      " [-0.67204021  3.51946916 -0.21742588 ...  0.57634728 -0.20382646\n",
      "  -0.20807853]\n",
      " ...\n",
      " [ 0.11959919 -0.67605037 -0.49972355 ...  0.57634728 -0.01879476\n",
      "  -0.23577368]\n",
      " [-0.18826058 -0.48534494 -0.21742588 ...  0.57634728 -0.06505269\n",
      "   0.20665959]\n",
      " [ 0.16357915 -0.67605037 -0.49972355 ...  0.57634728  1.8777802\n",
      "  -0.25087444]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "13308b1f-0c01-456e-96c8-e274f1ce915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dad6d245-2fc0-4f60-bfd6-1ae9625d0d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360511    18.0\n",
      "392362     5.3\n",
      "441510     0.3\n",
      "106802     1.0\n",
      "53        22.2\n",
      "          ... \n",
      "326281    26.6\n",
      "64228      0.9\n",
      "340331     2.9\n",
      "1465      53.4\n",
      "40621      4.8\n",
      "Name: fatcontent, Length: 156756, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "18699cd7-a5a4-436d-bde9-94e7735825ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "47126628-89ff-46f1-9f07-2554941bcd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(BATCH_SIZE_LR_1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cc552688-6246-4b6e-a496-0d044e6fc31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "33d4ee1c-370c-4dac-9133-2ee098419668",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([[1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0], [0.0], [0.0], [1.0], [1.0],\n",
    "    [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0ade8f3c-d0ac-4587-bcab-b00fc3b91b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0dfda08b-2e67-4e94-b7b9-11cce6236503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.815578818321228\n",
      "epoch:  1  loss:  0.8152337074279785\n",
      "epoch:  2  loss:  0.8148888945579529\n",
      "epoch:  3  loss:  0.8145442605018616\n",
      "epoch:  4  loss:  0.8141998052597046\n",
      "epoch:  5  loss:  0.8138555288314819\n",
      "epoch:  6  loss:  0.8135116100311279\n",
      "epoch:  7  loss:  0.8131678104400635\n",
      "epoch:  8  loss:  0.8128241896629333\n",
      "epoch:  9  loss:  0.8124808073043823\n",
      "epoch:  10  loss:  0.8121377229690552\n",
      "epoch:  11  loss:  0.8117947578430176\n",
      "epoch:  12  loss:  0.8114520907402039\n",
      "epoch:  13  loss:  0.8111095428466797\n",
      "epoch:  14  loss:  0.8107671737670898\n",
      "epoch:  15  loss:  0.8104250431060791\n",
      "epoch:  16  loss:  0.8100832104682922\n",
      "epoch:  17  loss:  0.8097414970397949\n",
      "epoch:  18  loss:  0.8094000220298767\n",
      "epoch:  19  loss:  0.8090588450431824\n",
      "epoch:  20  loss:  0.8087178468704224\n",
      "epoch:  21  loss:  0.8083770275115967\n",
      "epoch:  22  loss:  0.8080363869667053\n",
      "epoch:  23  loss:  0.8076959252357483\n",
      "epoch:  24  loss:  0.8073557019233704\n",
      "epoch:  25  loss:  0.8070157170295715\n",
      "epoch:  26  loss:  0.806675910949707\n",
      "epoch:  27  loss:  0.8063364028930664\n",
      "epoch:  28  loss:  0.8059970140457153\n",
      "epoch:  29  loss:  0.8056578636169434\n",
      "epoch:  30  loss:  0.8053188920021057\n",
      "epoch:  31  loss:  0.8049800395965576\n",
      "epoch:  32  loss:  0.8046416640281677\n",
      "epoch:  33  loss:  0.8043033480644226\n",
      "epoch:  34  loss:  0.8039652109146118\n",
      "epoch:  35  loss:  0.8036273121833801\n",
      "epoch:  36  loss:  0.803289532661438\n",
      "epoch:  37  loss:  0.8029521107673645\n",
      "epoch:  38  loss:  0.8026147484779358\n",
      "epoch:  39  loss:  0.802277684211731\n",
      "epoch:  40  loss:  0.8019408583641052\n",
      "epoch:  41  loss:  0.801604151725769\n",
      "epoch:  42  loss:  0.801267683506012\n",
      "epoch:  43  loss:  0.800931453704834\n",
      "epoch:  44  loss:  0.8005954623222351\n",
      "epoch:  45  loss:  0.800259530544281\n",
      "epoch:  46  loss:  0.7999239563941956\n",
      "epoch:  47  loss:  0.7995885014533997\n",
      "epoch:  48  loss:  0.7992533445358276\n",
      "epoch:  49  loss:  0.7989182472229004\n",
      "epoch:  50  loss:  0.7985835671424866\n",
      "epoch:  51  loss:  0.7982488870620728\n",
      "epoch:  52  loss:  0.797914445400238\n",
      "epoch:  53  loss:  0.7975803017616272\n",
      "epoch:  54  loss:  0.7972463369369507\n",
      "epoch:  55  loss:  0.7969125509262085\n",
      "epoch:  56  loss:  0.7965789437294006\n",
      "epoch:  57  loss:  0.7962455749511719\n",
      "epoch:  58  loss:  0.7959124445915222\n",
      "epoch:  59  loss:  0.7955794930458069\n",
      "epoch:  60  loss:  0.7952466607093811\n",
      "epoch:  61  loss:  0.7949142456054688\n",
      "epoch:  62  loss:  0.7945817708969116\n",
      "epoch:  63  loss:  0.7942496538162231\n",
      "epoch:  64  loss:  0.793917715549469\n",
      "epoch:  65  loss:  0.793586015701294\n",
      "epoch:  66  loss:  0.7932544946670532\n",
      "epoch:  67  loss:  0.7929231524467468\n",
      "epoch:  68  loss:  0.7925919890403748\n",
      "epoch:  69  loss:  0.7922610640525818\n",
      "epoch:  70  loss:  0.7919303774833679\n",
      "epoch:  71  loss:  0.7915998101234436\n",
      "epoch:  72  loss:  0.7912695407867432\n",
      "epoch:  73  loss:  0.7909393310546875\n",
      "epoch:  74  loss:  0.7906094789505005\n",
      "epoch:  75  loss:  0.7902799248695374\n",
      "epoch:  76  loss:  0.789950430393219\n",
      "epoch:  77  loss:  0.7896209955215454\n",
      "epoch:  78  loss:  0.7892919182777405\n",
      "epoch:  79  loss:  0.7889630198478699\n",
      "epoch:  80  loss:  0.7886343002319336\n",
      "epoch:  81  loss:  0.7883057594299316\n",
      "epoch:  82  loss:  0.7879774570465088\n",
      "epoch:  83  loss:  0.787649393081665\n",
      "epoch:  84  loss:  0.7873215079307556\n",
      "epoch:  85  loss:  0.7869938015937805\n",
      "epoch:  86  loss:  0.7866663336753845\n",
      "epoch:  87  loss:  0.7863390445709229\n",
      "epoch:  88  loss:  0.7860119342803955\n",
      "epoch:  89  loss:  0.7856850624084473\n",
      "epoch:  90  loss:  0.7853583693504333\n",
      "epoch:  91  loss:  0.7850318551063538\n",
      "epoch:  92  loss:  0.784705638885498\n",
      "epoch:  93  loss:  0.7843794226646423\n",
      "epoch:  94  loss:  0.7840535640716553\n",
      "epoch:  95  loss:  0.7837278842926025\n",
      "epoch:  96  loss:  0.7834024429321289\n",
      "epoch:  97  loss:  0.7830770015716553\n",
      "epoch:  98  loss:  0.7827519774436951\n",
      "epoch:  99  loss:  0.7824270129203796\n",
      "epoch:  100  loss:  0.7821022868156433\n",
      "epoch:  101  loss:  0.7817779183387756\n",
      "epoch:  102  loss:  0.7814536094665527\n",
      "epoch:  103  loss:  0.7811294794082642\n",
      "epoch:  104  loss:  0.7808055281639099\n",
      "epoch:  105  loss:  0.7804818749427795\n",
      "epoch:  106  loss:  0.780158281326294\n",
      "epoch:  107  loss:  0.7798349857330322\n",
      "epoch:  108  loss:  0.7795118093490601\n",
      "epoch:  109  loss:  0.779188871383667\n",
      "epoch:  110  loss:  0.778866171836853\n",
      "epoch:  111  loss:  0.7785437107086182\n",
      "epoch:  112  loss:  0.7782213091850281\n",
      "epoch:  113  loss:  0.7778991460800171\n",
      "epoch:  114  loss:  0.77757728099823\n",
      "epoch:  115  loss:  0.7772555351257324\n",
      "epoch:  116  loss:  0.7769342660903931\n",
      "epoch:  117  loss:  0.7766129970550537\n",
      "epoch:  118  loss:  0.7762919664382935\n",
      "epoch:  119  loss:  0.7759711742401123\n",
      "epoch:  120  loss:  0.7756505608558655\n",
      "epoch:  121  loss:  0.7753300666809082\n",
      "epoch:  122  loss:  0.7750098705291748\n",
      "epoch:  123  loss:  0.7746898531913757\n",
      "epoch:  124  loss:  0.7743701934814453\n",
      "epoch:  125  loss:  0.7740507125854492\n",
      "epoch:  126  loss:  0.773731529712677\n",
      "epoch:  127  loss:  0.7734124064445496\n",
      "epoch:  128  loss:  0.773093581199646\n",
      "epoch:  129  loss:  0.7727749347686768\n",
      "epoch:  130  loss:  0.7724565267562866\n",
      "epoch:  131  loss:  0.7721382975578308\n",
      "epoch:  132  loss:  0.7718202471733093\n",
      "epoch:  133  loss:  0.7715024352073669\n",
      "epoch:  134  loss:  0.7711847424507141\n",
      "epoch:  135  loss:  0.7708672881126404\n",
      "epoch:  136  loss:  0.7705499529838562\n",
      "epoch:  137  loss:  0.7702328562736511\n",
      "epoch:  138  loss:  0.7699159979820251\n",
      "epoch:  139  loss:  0.7695992588996887\n",
      "epoch:  140  loss:  0.7692827582359314\n",
      "epoch:  141  loss:  0.7689663767814636\n",
      "epoch:  142  loss:  0.7686502933502197\n",
      "epoch:  143  loss:  0.7683343887329102\n",
      "epoch:  144  loss:  0.7680184841156006\n",
      "epoch:  145  loss:  0.7677030563354492\n",
      "epoch:  146  loss:  0.7673876285552979\n",
      "epoch:  147  loss:  0.7670724987983704\n",
      "epoch:  148  loss:  0.7667574882507324\n",
      "epoch:  149  loss:  0.7664427161216736\n",
      "epoch:  150  loss:  0.7661281228065491\n",
      "epoch:  151  loss:  0.7658135890960693\n",
      "epoch:  152  loss:  0.765499472618103\n",
      "epoch:  153  loss:  0.7651854753494263\n",
      "epoch:  154  loss:  0.7648716568946838\n",
      "epoch:  155  loss:  0.7645580172538757\n",
      "epoch:  156  loss:  0.764244556427002\n",
      "epoch:  157  loss:  0.7639314532279968\n",
      "epoch:  158  loss:  0.7636182904243469\n",
      "epoch:  159  loss:  0.7633054256439209\n",
      "epoch:  160  loss:  0.7629928588867188\n",
      "epoch:  161  loss:  0.7626803517341614\n",
      "epoch:  162  loss:  0.7623680233955383\n",
      "epoch:  163  loss:  0.7620559930801392\n",
      "epoch:  164  loss:  0.7617440819740295\n",
      "epoch:  165  loss:  0.7614322900772095\n",
      "epoch:  166  loss:  0.7611207962036133\n",
      "epoch:  167  loss:  0.7608094215393066\n",
      "epoch:  168  loss:  0.7604982852935791\n",
      "epoch:  169  loss:  0.7601872682571411\n",
      "epoch:  170  loss:  0.7598764300346375\n",
      "epoch:  171  loss:  0.7595659494400024\n",
      "epoch:  172  loss:  0.7592554688453674\n",
      "epoch:  173  loss:  0.7589452266693115\n",
      "epoch:  174  loss:  0.7586351633071899\n",
      "epoch:  175  loss:  0.7583253383636475\n",
      "epoch:  176  loss:  0.7580157518386841\n",
      "epoch:  177  loss:  0.7577062249183655\n",
      "epoch:  178  loss:  0.7573968768119812\n",
      "epoch:  179  loss:  0.7570878267288208\n",
      "epoch:  180  loss:  0.7567788362503052\n",
      "epoch:  181  loss:  0.7564701437950134\n",
      "epoch:  182  loss:  0.756161630153656\n",
      "epoch:  183  loss:  0.7558531761169434\n",
      "epoch:  184  loss:  0.7555450201034546\n",
      "epoch:  185  loss:  0.7552370429039001\n",
      "epoch:  186  loss:  0.75492924451828\n",
      "epoch:  187  loss:  0.7546216249465942\n",
      "epoch:  188  loss:  0.7543141841888428\n",
      "epoch:  189  loss:  0.7540069222450256\n",
      "epoch:  190  loss:  0.7536998391151428\n",
      "epoch:  191  loss:  0.7533929944038391\n",
      "epoch:  192  loss:  0.753086268901825\n",
      "epoch:  193  loss:  0.7527797818183899\n",
      "epoch:  194  loss:  0.7524733543395996\n",
      "epoch:  195  loss:  0.7521672248840332\n",
      "epoch:  196  loss:  0.7518613338470459\n",
      "epoch:  197  loss:  0.7515555024147034\n",
      "epoch:  198  loss:  0.7512499690055847\n",
      "epoch:  199  loss:  0.7509444952011108\n",
      "epoch:  200  loss:  0.7506392002105713\n",
      "epoch:  201  loss:  0.7503342628479004\n",
      "epoch:  202  loss:  0.750029444694519\n",
      "epoch:  203  loss:  0.7497246861457825\n",
      "epoch:  204  loss:  0.749420166015625\n",
      "epoch:  205  loss:  0.7491158246994019\n",
      "epoch:  206  loss:  0.7488118410110474\n",
      "epoch:  207  loss:  0.748508095741272\n",
      "epoch:  208  loss:  0.7482044696807861\n",
      "epoch:  209  loss:  0.7479010224342346\n",
      "epoch:  210  loss:  0.7475979328155518\n",
      "epoch:  211  loss:  0.7472950220108032\n",
      "epoch:  212  loss:  0.7469923496246338\n",
      "epoch:  213  loss:  0.7466897368431091\n",
      "epoch:  214  loss:  0.7463873624801636\n",
      "epoch:  215  loss:  0.7460851669311523\n",
      "epoch:  216  loss:  0.7457830905914307\n",
      "epoch:  217  loss:  0.7454812526702881\n",
      "epoch:  218  loss:  0.7451795935630798\n",
      "epoch:  219  loss:  0.7448781728744507\n",
      "epoch:  220  loss:  0.7445768117904663\n",
      "epoch:  221  loss:  0.744275689125061\n",
      "epoch:  222  loss:  0.7439747452735901\n",
      "epoch:  223  loss:  0.7436739802360535\n",
      "epoch:  224  loss:  0.7433733940124512\n",
      "epoch:  225  loss:  0.743073046207428\n",
      "epoch:  226  loss:  0.7427728176116943\n",
      "epoch:  227  loss:  0.7424726486206055\n",
      "epoch:  228  loss:  0.7421728372573853\n",
      "epoch:  229  loss:  0.7418732047080994\n",
      "epoch:  230  loss:  0.7415736317634583\n",
      "epoch:  231  loss:  0.7412744164466858\n",
      "epoch:  232  loss:  0.7409752011299133\n",
      "epoch:  233  loss:  0.7406762838363647\n",
      "epoch:  234  loss:  0.7403773665428162\n",
      "epoch:  235  loss:  0.740078866481781\n",
      "epoch:  236  loss:  0.7397804260253906\n",
      "epoch:  237  loss:  0.7394821643829346\n",
      "epoch:  238  loss:  0.7391841411590576\n",
      "epoch:  239  loss:  0.7388861775398254\n",
      "epoch:  240  loss:  0.7385885715484619\n",
      "epoch:  241  loss:  0.7382909059524536\n",
      "epoch:  242  loss:  0.7379935383796692\n",
      "epoch:  243  loss:  0.7376962900161743\n",
      "epoch:  244  loss:  0.7373992800712585\n",
      "epoch:  245  loss:  0.7371023893356323\n",
      "epoch:  246  loss:  0.73680579662323\n",
      "epoch:  247  loss:  0.736509382724762\n",
      "epoch:  248  loss:  0.736212968826294\n",
      "epoch:  249  loss:  0.7359169125556946\n",
      "epoch:  250  loss:  0.73562091588974\n",
      "epoch:  251  loss:  0.7353251576423645\n",
      "epoch:  252  loss:  0.7350295186042786\n",
      "epoch:  253  loss:  0.7347341179847717\n",
      "epoch:  254  loss:  0.7344388365745544\n",
      "epoch:  255  loss:  0.7341437935829163\n",
      "epoch:  256  loss:  0.7338489294052124\n",
      "epoch:  257  loss:  0.7335541844367981\n",
      "epoch:  258  loss:  0.7332596778869629\n",
      "epoch:  259  loss:  0.7329657673835754\n",
      "epoch:  260  loss:  0.732672929763794\n",
      "epoch:  261  loss:  0.7323802709579468\n",
      "epoch:  262  loss:  0.7320878505706787\n",
      "epoch:  263  loss:  0.7317955493927002\n",
      "epoch:  264  loss:  0.7315033674240112\n",
      "epoch:  265  loss:  0.7312114834785461\n",
      "epoch:  266  loss:  0.7309197783470154\n",
      "epoch:  267  loss:  0.7306280732154846\n",
      "epoch:  268  loss:  0.7303367257118225\n",
      "epoch:  269  loss:  0.7300454378128052\n",
      "epoch:  270  loss:  0.7297543287277222\n",
      "epoch:  271  loss:  0.7294634580612183\n",
      "epoch:  272  loss:  0.7291727662086487\n",
      "epoch:  273  loss:  0.7288821935653687\n",
      "epoch:  274  loss:  0.728591799736023\n",
      "epoch:  275  loss:  0.7283021211624146\n",
      "epoch:  276  loss:  0.7280132174491882\n",
      "epoch:  277  loss:  0.7277246713638306\n",
      "epoch:  278  loss:  0.7274362444877625\n",
      "epoch:  279  loss:  0.7271479368209839\n",
      "epoch:  280  loss:  0.7268598675727844\n",
      "epoch:  281  loss:  0.7265720367431641\n",
      "epoch:  282  loss:  0.7262842655181885\n",
      "epoch:  283  loss:  0.7259966731071472\n",
      "epoch:  284  loss:  0.7257091999053955\n",
      "epoch:  285  loss:  0.7254219651222229\n",
      "epoch:  286  loss:  0.7251350283622742\n",
      "epoch:  287  loss:  0.7248480916023254\n",
      "epoch:  288  loss:  0.7245613932609558\n",
      "epoch:  289  loss:  0.7242748141288757\n",
      "epoch:  290  loss:  0.7239884734153748\n",
      "epoch:  291  loss:  0.7237022519111633\n",
      "epoch:  292  loss:  0.7234162092208862\n",
      "epoch:  293  loss:  0.7231304049491882\n",
      "epoch:  294  loss:  0.7228446006774902\n",
      "epoch:  295  loss:  0.7225590944290161\n",
      "epoch:  296  loss:  0.7222737073898315\n",
      "epoch:  297  loss:  0.7219885587692261\n",
      "epoch:  298  loss:  0.7217034101486206\n",
      "epoch:  299  loss:  0.7214186191558838\n",
      "epoch:  300  loss:  0.7211339473724365\n",
      "epoch:  301  loss:  0.720849335193634\n",
      "epoch:  302  loss:  0.7205650210380554\n",
      "epoch:  303  loss:  0.7202808856964111\n",
      "epoch:  304  loss:  0.7199968099594116\n",
      "epoch:  305  loss:  0.7197129726409912\n",
      "epoch:  306  loss:  0.7194291949272156\n",
      "epoch:  307  loss:  0.7191457748413086\n",
      "epoch:  308  loss:  0.7188624739646912\n",
      "epoch:  309  loss:  0.7185792922973633\n",
      "epoch:  310  loss:  0.7182961106300354\n",
      "epoch:  311  loss:  0.718013346195221\n",
      "epoch:  312  loss:  0.7177306413650513\n",
      "epoch:  313  loss:  0.7174480557441711\n",
      "epoch:  314  loss:  0.7171657085418701\n",
      "epoch:  315  loss:  0.7168835401535034\n",
      "epoch:  316  loss:  0.7166013717651367\n",
      "epoch:  317  loss:  0.7163190841674805\n",
      "epoch:  318  loss:  0.7160371541976929\n",
      "epoch:  319  loss:  0.7157553434371948\n",
      "epoch:  320  loss:  0.7154737114906311\n",
      "epoch:  321  loss:  0.7151921987533569\n",
      "epoch:  322  loss:  0.7149108648300171\n",
      "epoch:  323  loss:  0.7146297097206116\n",
      "epoch:  324  loss:  0.7143487334251404\n",
      "epoch:  325  loss:  0.7140678763389587\n",
      "epoch:  326  loss:  0.7137872576713562\n",
      "epoch:  327  loss:  0.7135066986083984\n",
      "epoch:  328  loss:  0.7132263779640198\n",
      "epoch:  329  loss:  0.7129461765289307\n",
      "epoch:  330  loss:  0.7126661539077759\n",
      "epoch:  331  loss:  0.7123863697052002\n",
      "epoch:  332  loss:  0.7121067047119141\n",
      "epoch:  333  loss:  0.7118272185325623\n",
      "epoch:  334  loss:  0.7115478515625\n",
      "epoch:  335  loss:  0.7112686634063721\n",
      "epoch:  336  loss:  0.7109896540641785\n",
      "epoch:  337  loss:  0.7107108235359192\n",
      "epoch:  338  loss:  0.7104321122169495\n",
      "epoch:  339  loss:  0.7101535797119141\n",
      "epoch:  340  loss:  0.7098751664161682\n",
      "epoch:  341  loss:  0.7095969915390015\n",
      "epoch:  342  loss:  0.709318995475769\n",
      "epoch:  343  loss:  0.7090411186218262\n",
      "epoch:  344  loss:  0.7087634801864624\n",
      "epoch:  345  loss:  0.7084859609603882\n",
      "epoch:  346  loss:  0.7082085013389587\n",
      "epoch:  347  loss:  0.7079312801361084\n",
      "epoch:  348  loss:  0.7076542377471924\n",
      "epoch:  349  loss:  0.7073772549629211\n",
      "epoch:  350  loss:  0.7071005702018738\n",
      "epoch:  351  loss:  0.7068240642547607\n",
      "epoch:  352  loss:  0.7065476179122925\n",
      "epoch:  353  loss:  0.7062714099884033\n",
      "epoch:  354  loss:  0.7059953212738037\n",
      "epoch:  355  loss:  0.7057194113731384\n",
      "epoch:  356  loss:  0.7054435610771179\n",
      "epoch:  357  loss:  0.7051680088043213\n",
      "epoch:  358  loss:  0.7048925757408142\n",
      "epoch:  359  loss:  0.7046172618865967\n",
      "epoch:  360  loss:  0.7043421864509583\n",
      "epoch:  361  loss:  0.7040672898292542\n",
      "epoch:  362  loss:  0.7037925124168396\n",
      "epoch:  363  loss:  0.7035177946090698\n",
      "epoch:  364  loss:  0.7032433748245239\n",
      "epoch:  365  loss:  0.7029690146446228\n",
      "epoch:  366  loss:  0.7026947736740112\n",
      "epoch:  367  loss:  0.7024208307266235\n",
      "epoch:  368  loss:  0.7021470069885254\n",
      "epoch:  369  loss:  0.7018733024597168\n",
      "epoch:  370  loss:  0.7015997171401978\n",
      "epoch:  371  loss:  0.7013264298439026\n",
      "epoch:  372  loss:  0.7010532021522522\n",
      "epoch:  373  loss:  0.7007800936698914\n",
      "epoch:  374  loss:  0.7005072236061096\n",
      "epoch:  375  loss:  0.7002344727516174\n",
      "epoch:  376  loss:  0.6999619007110596\n",
      "epoch:  377  loss:  0.6996895670890808\n",
      "epoch:  378  loss:  0.6994173526763916\n",
      "epoch:  379  loss:  0.6991451978683472\n",
      "epoch:  380  loss:  0.6988732814788818\n",
      "epoch:  381  loss:  0.6986016035079956\n",
      "epoch:  382  loss:  0.6983298659324646\n",
      "epoch:  383  loss:  0.6980584263801575\n",
      "epoch:  384  loss:  0.6977871656417847\n",
      "epoch:  385  loss:  0.6975160241127014\n",
      "epoch:  386  loss:  0.6972451210021973\n",
      "epoch:  387  loss:  0.6969742774963379\n",
      "epoch:  388  loss:  0.6967036724090576\n",
      "epoch:  389  loss:  0.6964333057403564\n",
      "epoch:  390  loss:  0.6961629390716553\n",
      "epoch:  391  loss:  0.6958927512168884\n",
      "epoch:  392  loss:  0.6956228017807007\n",
      "epoch:  393  loss:  0.6953529715538025\n",
      "epoch:  394  loss:  0.6950833797454834\n",
      "epoch:  395  loss:  0.6948138475418091\n",
      "epoch:  396  loss:  0.6945444941520691\n",
      "epoch:  397  loss:  0.6942751407623291\n",
      "epoch:  398  loss:  0.6940062642097473\n",
      "epoch:  399  loss:  0.693737268447876\n",
      "epoch:  400  loss:  0.6934686303138733\n",
      "epoch:  401  loss:  0.6932000517845154\n",
      "epoch:  402  loss:  0.6929316520690918\n",
      "epoch:  403  loss:  0.6926633715629578\n",
      "epoch:  404  loss:  0.6923952698707581\n",
      "epoch:  405  loss:  0.6921273469924927\n",
      "epoch:  406  loss:  0.6918595433235168\n",
      "epoch:  407  loss:  0.6915918588638306\n",
      "epoch:  408  loss:  0.6913244128227234\n",
      "epoch:  409  loss:  0.691057026386261\n",
      "epoch:  410  loss:  0.6907899975776672\n",
      "epoch:  411  loss:  0.6905229091644287\n",
      "epoch:  412  loss:  0.6902561187744141\n",
      "epoch:  413  loss:  0.6899893879890442\n",
      "epoch:  414  loss:  0.6897228956222534\n",
      "epoch:  415  loss:  0.6894564628601074\n",
      "epoch:  416  loss:  0.6891902685165405\n",
      "epoch:  417  loss:  0.6889240741729736\n",
      "epoch:  418  loss:  0.6886582970619202\n",
      "epoch:  419  loss:  0.6883925199508667\n",
      "epoch:  420  loss:  0.6881269216537476\n",
      "epoch:  421  loss:  0.6878613829612732\n",
      "epoch:  422  loss:  0.6875961422920227\n",
      "epoch:  423  loss:  0.687330961227417\n",
      "epoch:  424  loss:  0.6870660185813904\n",
      "epoch:  425  loss:  0.6868011951446533\n",
      "epoch:  426  loss:  0.6865364909172058\n",
      "epoch:  427  loss:  0.6862719655036926\n",
      "epoch:  428  loss:  0.6860076189041138\n",
      "epoch:  429  loss:  0.6857433319091797\n",
      "epoch:  430  loss:  0.6854793429374695\n",
      "epoch:  431  loss:  0.6852153539657593\n",
      "epoch:  432  loss:  0.684951663017273\n",
      "epoch:  433  loss:  0.6846880316734314\n",
      "epoch:  434  loss:  0.6844245791435242\n",
      "epoch:  435  loss:  0.684161365032196\n",
      "epoch:  436  loss:  0.6838981509208679\n",
      "epoch:  437  loss:  0.6836351156234741\n",
      "epoch:  438  loss:  0.6833723783493042\n",
      "epoch:  439  loss:  0.6831096410751343\n",
      "epoch:  440  loss:  0.6828470826148987\n",
      "epoch:  441  loss:  0.6825847625732422\n",
      "epoch:  442  loss:  0.68232262134552\n",
      "epoch:  443  loss:  0.6820604801177979\n",
      "epoch:  444  loss:  0.6817985773086548\n",
      "epoch:  445  loss:  0.681536853313446\n",
      "epoch:  446  loss:  0.6812751293182373\n",
      "epoch:  447  loss:  0.6810137033462524\n",
      "epoch:  448  loss:  0.6807523965835571\n",
      "epoch:  449  loss:  0.6804912686347961\n",
      "epoch:  450  loss:  0.6802302598953247\n",
      "epoch:  451  loss:  0.6799693703651428\n",
      "epoch:  452  loss:  0.67970871925354\n",
      "epoch:  453  loss:  0.679448127746582\n",
      "epoch:  454  loss:  0.6791877150535583\n",
      "epoch:  455  loss:  0.678927481174469\n",
      "epoch:  456  loss:  0.678667426109314\n",
      "epoch:  457  loss:  0.6784074306488037\n",
      "epoch:  458  loss:  0.6781476736068726\n",
      "epoch:  459  loss:  0.677888035774231\n",
      "epoch:  460  loss:  0.6776285171508789\n",
      "epoch:  461  loss:  0.6773693561553955\n",
      "epoch:  462  loss:  0.6771102547645569\n",
      "epoch:  463  loss:  0.6768512725830078\n",
      "epoch:  464  loss:  0.6765924692153931\n",
      "epoch:  465  loss:  0.6763338446617126\n",
      "epoch:  466  loss:  0.676075279712677\n",
      "epoch:  467  loss:  0.6758168935775757\n",
      "epoch:  468  loss:  0.6755586862564087\n",
      "epoch:  469  loss:  0.6753007173538208\n",
      "epoch:  470  loss:  0.6750428080558777\n",
      "epoch:  471  loss:  0.6747850179672241\n",
      "epoch:  472  loss:  0.6745274066925049\n",
      "epoch:  473  loss:  0.67426997423172\n",
      "epoch:  474  loss:  0.6740126609802246\n",
      "epoch:  475  loss:  0.6737555861473083\n",
      "epoch:  476  loss:  0.6734984517097473\n",
      "epoch:  477  loss:  0.6732416152954102\n",
      "epoch:  478  loss:  0.6729848980903625\n",
      "epoch:  479  loss:  0.6727283596992493\n",
      "epoch:  480  loss:  0.6724719405174255\n",
      "epoch:  481  loss:  0.6722157001495361\n",
      "epoch:  482  loss:  0.671959638595581\n",
      "epoch:  483  loss:  0.6717036366462708\n",
      "epoch:  484  loss:  0.6714478135108948\n",
      "epoch:  485  loss:  0.6711920499801636\n",
      "epoch:  486  loss:  0.670936644077301\n",
      "epoch:  487  loss:  0.6706811785697937\n",
      "epoch:  488  loss:  0.6704260110855103\n",
      "epoch:  489  loss:  0.6701709032058716\n",
      "epoch:  490  loss:  0.669916033744812\n",
      "epoch:  491  loss:  0.6696611642837524\n",
      "epoch:  492  loss:  0.669406533241272\n",
      "epoch:  493  loss:  0.6691520810127258\n",
      "epoch:  494  loss:  0.668897807598114\n",
      "epoch:  495  loss:  0.668643593788147\n",
      "epoch:  496  loss:  0.6683896780014038\n",
      "epoch:  497  loss:  0.6681357026100159\n",
      "epoch:  498  loss:  0.6678819060325623\n",
      "epoch:  499  loss:  0.667628288269043\n",
      "epoch:  500  loss:  0.667374849319458\n",
      "epoch:  501  loss:  0.6671215295791626\n",
      "epoch:  502  loss:  0.6668683886528015\n",
      "epoch:  503  loss:  0.66661536693573\n",
      "epoch:  504  loss:  0.6663625240325928\n",
      "epoch:  505  loss:  0.6661098003387451\n",
      "epoch:  506  loss:  0.6658571362495422\n",
      "epoch:  507  loss:  0.665604829788208\n",
      "epoch:  508  loss:  0.6653525233268738\n",
      "epoch:  509  loss:  0.6651004552841187\n",
      "epoch:  510  loss:  0.6648484468460083\n",
      "epoch:  511  loss:  0.6645965576171875\n",
      "epoch:  512  loss:  0.664344847202301\n",
      "epoch:  513  loss:  0.6640933156013489\n",
      "epoch:  514  loss:  0.663841962814331\n",
      "epoch:  515  loss:  0.6635906100273132\n",
      "epoch:  516  loss:  0.6633395552635193\n",
      "epoch:  517  loss:  0.6630886197090149\n",
      "epoch:  518  loss:  0.6628378033638\n",
      "epoch:  519  loss:  0.6625871062278748\n",
      "epoch:  520  loss:  0.6623365879058838\n",
      "epoch:  521  loss:  0.6620861887931824\n",
      "epoch:  522  loss:  0.6618360280990601\n",
      "epoch:  523  loss:  0.6615858674049377\n",
      "epoch:  524  loss:  0.6613358855247498\n",
      "epoch:  525  loss:  0.6610860824584961\n",
      "epoch:  526  loss:  0.660836398601532\n",
      "epoch:  527  loss:  0.6605868339538574\n",
      "epoch:  528  loss:  0.6603374481201172\n",
      "epoch:  529  loss:  0.6600881814956665\n",
      "epoch:  530  loss:  0.6598392128944397\n",
      "epoch:  531  loss:  0.6595901846885681\n",
      "epoch:  532  loss:  0.6593414545059204\n",
      "epoch:  533  loss:  0.6590927243232727\n",
      "epoch:  534  loss:  0.6588442325592041\n",
      "epoch:  535  loss:  0.6585958003997803\n",
      "epoch:  536  loss:  0.6583476066589355\n",
      "epoch:  537  loss:  0.6580995321273804\n",
      "epoch:  538  loss:  0.6578515768051147\n",
      "epoch:  539  loss:  0.6576038599014282\n",
      "epoch:  540  loss:  0.6573562026023865\n",
      "epoch:  541  loss:  0.6571086049079895\n",
      "epoch:  542  loss:  0.6568612456321716\n",
      "epoch:  543  loss:  0.6566140651702881\n",
      "epoch:  544  loss:  0.6563670039176941\n",
      "epoch:  545  loss:  0.6561200618743896\n",
      "epoch:  546  loss:  0.6558732986450195\n",
      "epoch:  547  loss:  0.655626654624939\n",
      "epoch:  548  loss:  0.655380129814148\n",
      "epoch:  549  loss:  0.6551336646080017\n",
      "epoch:  550  loss:  0.6548874974250793\n",
      "epoch:  551  loss:  0.6546413898468018\n",
      "epoch:  552  loss:  0.6543954610824585\n",
      "epoch:  553  loss:  0.6541496515274048\n",
      "epoch:  554  loss:  0.6539039611816406\n",
      "epoch:  555  loss:  0.6536585092544556\n",
      "epoch:  556  loss:  0.6534130573272705\n",
      "epoch:  557  loss:  0.6531678438186646\n",
      "epoch:  558  loss:  0.6529226899147034\n",
      "epoch:  559  loss:  0.6526778340339661\n",
      "epoch:  560  loss:  0.6524326205253601\n",
      "epoch:  561  loss:  0.652187705039978\n",
      "epoch:  562  loss:  0.6519428491592407\n",
      "epoch:  563  loss:  0.6516982316970825\n",
      "epoch:  564  loss:  0.6514536142349243\n",
      "epoch:  565  loss:  0.6512093544006348\n",
      "epoch:  566  loss:  0.6509650349617004\n",
      "epoch:  567  loss:  0.6507209539413452\n",
      "epoch:  568  loss:  0.6504769921302795\n",
      "epoch:  569  loss:  0.6502331495285034\n",
      "epoch:  570  loss:  0.6499896049499512\n",
      "epoch:  571  loss:  0.6497460007667542\n",
      "epoch:  572  loss:  0.6495025157928467\n",
      "epoch:  573  loss:  0.6492593288421631\n",
      "epoch:  574  loss:  0.6490162014961243\n",
      "epoch:  575  loss:  0.6487732529640198\n",
      "epoch:  576  loss:  0.6485303044319153\n",
      "epoch:  577  loss:  0.6482876539230347\n",
      "epoch:  578  loss:  0.6480451226234436\n",
      "epoch:  579  loss:  0.6478027105331421\n",
      "epoch:  580  loss:  0.6475604772567749\n",
      "epoch:  581  loss:  0.6473182439804077\n",
      "epoch:  582  loss:  0.6470763683319092\n",
      "epoch:  583  loss:  0.6468344926834106\n",
      "epoch:  584  loss:  0.6465927362442017\n",
      "epoch:  585  loss:  0.646351158618927\n",
      "epoch:  586  loss:  0.6461096405982971\n",
      "epoch:  587  loss:  0.6458684206008911\n",
      "epoch:  588  loss:  0.6456272602081299\n",
      "epoch:  589  loss:  0.6453862190246582\n",
      "epoch:  590  loss:  0.6451454162597656\n",
      "epoch:  591  loss:  0.6449045538902283\n",
      "epoch:  592  loss:  0.6446640491485596\n",
      "epoch:  593  loss:  0.6444236040115356\n",
      "epoch:  594  loss:  0.644183337688446\n",
      "epoch:  595  loss:  0.6439430713653564\n",
      "epoch:  596  loss:  0.643703043460846\n",
      "epoch:  597  loss:  0.6434631943702698\n",
      "epoch:  598  loss:  0.6432234048843384\n",
      "epoch:  599  loss:  0.6429837942123413\n",
      "epoch:  600  loss:  0.6427443027496338\n",
      "epoch:  601  loss:  0.6425049304962158\n",
      "epoch:  602  loss:  0.6422656774520874\n",
      "epoch:  603  loss:  0.6420266628265381\n",
      "epoch:  604  loss:  0.6417877674102783\n",
      "epoch:  605  loss:  0.6415488719940186\n",
      "epoch:  606  loss:  0.6413102746009827\n",
      "epoch:  607  loss:  0.6410717964172363\n",
      "epoch:  608  loss:  0.6408334374427795\n",
      "epoch:  609  loss:  0.6405951380729675\n",
      "epoch:  610  loss:  0.6403570771217346\n",
      "epoch:  611  loss:  0.6401190757751465\n",
      "epoch:  612  loss:  0.6398812532424927\n",
      "epoch:  613  loss:  0.6396436095237732\n",
      "epoch:  614  loss:  0.6394060254096985\n",
      "epoch:  615  loss:  0.6391686201095581\n",
      "epoch:  616  loss:  0.6389313340187073\n",
      "epoch:  617  loss:  0.6386941075325012\n",
      "epoch:  618  loss:  0.6384571194648743\n",
      "epoch:  619  loss:  0.6382202506065369\n",
      "epoch:  620  loss:  0.637983500957489\n",
      "epoch:  621  loss:  0.6377468705177307\n",
      "epoch:  622  loss:  0.6375102400779724\n",
      "epoch:  623  loss:  0.637273907661438\n",
      "epoch:  624  loss:  0.6370375752449036\n",
      "epoch:  625  loss:  0.6368014812469482\n",
      "epoch:  626  loss:  0.6365654468536377\n",
      "epoch:  627  loss:  0.6363295316696167\n",
      "epoch:  628  loss:  0.6360938549041748\n",
      "epoch:  629  loss:  0.6358582377433777\n",
      "epoch:  630  loss:  0.6356227993965149\n",
      "epoch:  631  loss:  0.6353874802589417\n",
      "epoch:  632  loss:  0.6351522207260132\n",
      "epoch:  633  loss:  0.6349171996116638\n",
      "epoch:  634  loss:  0.6346823573112488\n",
      "epoch:  635  loss:  0.6344475150108337\n",
      "epoch:  636  loss:  0.6342127919197083\n",
      "epoch:  637  loss:  0.6339783072471619\n",
      "epoch:  638  loss:  0.633743941783905\n",
      "epoch:  639  loss:  0.6335097551345825\n",
      "epoch:  640  loss:  0.6332756280899048\n",
      "epoch:  641  loss:  0.6330417394638062\n",
      "epoch:  642  loss:  0.6328078508377075\n",
      "epoch:  643  loss:  0.6325741410255432\n",
      "epoch:  644  loss:  0.6323406100273132\n",
      "epoch:  645  loss:  0.6321070194244385\n",
      "epoch:  646  loss:  0.6318738460540771\n",
      "epoch:  647  loss:  0.6316406726837158\n",
      "epoch:  648  loss:  0.631407618522644\n",
      "epoch:  649  loss:  0.6311747431755066\n",
      "epoch:  650  loss:  0.6309419870376587\n",
      "epoch:  651  loss:  0.6307093501091003\n",
      "epoch:  652  loss:  0.6304768323898315\n",
      "epoch:  653  loss:  0.6302445530891418\n",
      "epoch:  654  loss:  0.6300122737884521\n",
      "epoch:  655  loss:  0.6297801733016968\n",
      "epoch:  656  loss:  0.629548192024231\n",
      "epoch:  657  loss:  0.6293164491653442\n",
      "epoch:  658  loss:  0.629084587097168\n",
      "epoch:  659  loss:  0.6288526654243469\n",
      "epoch:  660  loss:  0.6286208033561707\n",
      "epoch:  661  loss:  0.6283892393112183\n",
      "epoch:  662  loss:  0.6281576752662659\n",
      "epoch:  663  loss:  0.627926230430603\n",
      "epoch:  664  loss:  0.6276944875717163\n",
      "epoch:  665  loss:  0.6274628639221191\n",
      "epoch:  666  loss:  0.6272313594818115\n",
      "epoch:  667  loss:  0.6270000338554382\n",
      "epoch:  668  loss:  0.6267688870429993\n",
      "epoch:  669  loss:  0.6265377998352051\n",
      "epoch:  670  loss:  0.6263067722320557\n",
      "epoch:  671  loss:  0.6260759830474854\n",
      "epoch:  672  loss:  0.6258453130722046\n",
      "epoch:  673  loss:  0.6256147623062134\n",
      "epoch:  674  loss:  0.6253843903541565\n",
      "epoch:  675  loss:  0.6251542568206787\n",
      "epoch:  676  loss:  0.6249250173568726\n",
      "epoch:  677  loss:  0.6246960163116455\n",
      "epoch:  678  loss:  0.624467134475708\n",
      "epoch:  679  loss:  0.6242382526397705\n",
      "epoch:  680  loss:  0.6240096092224121\n",
      "epoch:  681  loss:  0.623781144618988\n",
      "epoch:  682  loss:  0.6235527396202087\n",
      "epoch:  683  loss:  0.6233245134353638\n",
      "epoch:  684  loss:  0.6230964064598083\n",
      "epoch:  685  loss:  0.6228684186935425\n",
      "epoch:  686  loss:  0.6226405501365662\n",
      "epoch:  687  loss:  0.6224128007888794\n",
      "epoch:  688  loss:  0.622185230255127\n",
      "epoch:  689  loss:  0.6219577193260193\n",
      "epoch:  690  loss:  0.621730387210846\n",
      "epoch:  691  loss:  0.6215031743049622\n",
      "epoch:  692  loss:  0.6212761998176575\n",
      "epoch:  693  loss:  0.621049165725708\n",
      "epoch:  694  loss:  0.6208223700523376\n",
      "epoch:  695  loss:  0.6205956935882568\n",
      "epoch:  696  loss:  0.6203691363334656\n",
      "epoch:  697  loss:  0.6201426982879639\n",
      "epoch:  698  loss:  0.6199164390563965\n",
      "epoch:  699  loss:  0.6196902990341187\n",
      "epoch:  700  loss:  0.6194641590118408\n",
      "epoch:  701  loss:  0.6192382574081421\n",
      "epoch:  702  loss:  0.6190125942230225\n",
      "epoch:  703  loss:  0.6187869310379028\n",
      "epoch:  704  loss:  0.6185613870620728\n",
      "epoch:  705  loss:  0.6183359622955322\n",
      "epoch:  706  loss:  0.618110716342926\n",
      "epoch:  707  loss:  0.6178855895996094\n",
      "epoch:  708  loss:  0.6176608800888062\n",
      "epoch:  709  loss:  0.6174368262290955\n",
      "epoch:  710  loss:  0.6172130107879639\n",
      "epoch:  711  loss:  0.616989254951477\n",
      "epoch:  712  loss:  0.6167656779289246\n",
      "epoch:  713  loss:  0.6165422201156616\n",
      "epoch:  714  loss:  0.6163188815116882\n",
      "epoch:  715  loss:  0.6160956621170044\n",
      "epoch:  716  loss:  0.6158725619316101\n",
      "epoch:  717  loss:  0.6156497001647949\n",
      "epoch:  718  loss:  0.6154268383979797\n",
      "epoch:  719  loss:  0.6152041554450989\n",
      "epoch:  720  loss:  0.6149815320968628\n",
      "epoch:  721  loss:  0.614759087562561\n",
      "epoch:  722  loss:  0.6145367622375488\n",
      "epoch:  723  loss:  0.6143145561218262\n",
      "epoch:  724  loss:  0.6140924692153931\n",
      "epoch:  725  loss:  0.6138708591461182\n",
      "epoch:  726  loss:  0.6136499643325806\n",
      "epoch:  727  loss:  0.6134291291236877\n",
      "epoch:  728  loss:  0.6132084727287292\n",
      "epoch:  729  loss:  0.6129879951477051\n",
      "epoch:  730  loss:  0.6127674579620361\n",
      "epoch:  731  loss:  0.6125471591949463\n",
      "epoch:  732  loss:  0.6123270988464355\n",
      "epoch:  733  loss:  0.6121070384979248\n",
      "epoch:  734  loss:  0.6118871569633484\n",
      "epoch:  735  loss:  0.6116673946380615\n",
      "epoch:  736  loss:  0.6114476919174194\n",
      "epoch:  737  loss:  0.6112281680107117\n",
      "epoch:  738  loss:  0.6110087633132935\n",
      "epoch:  739  loss:  0.6107894778251648\n",
      "epoch:  740  loss:  0.6105703711509705\n",
      "epoch:  741  loss:  0.6103513240814209\n",
      "epoch:  742  loss:  0.6101323962211609\n",
      "epoch:  743  loss:  0.6099135279655457\n",
      "epoch:  744  loss:  0.6096949577331543\n",
      "epoch:  745  loss:  0.6094763875007629\n",
      "epoch:  746  loss:  0.6092579960823059\n",
      "epoch:  747  loss:  0.6090396642684937\n",
      "epoch:  748  loss:  0.608821451663971\n",
      "epoch:  749  loss:  0.6086034178733826\n",
      "epoch:  750  loss:  0.6083855628967285\n",
      "epoch:  751  loss:  0.6081677079200745\n",
      "epoch:  752  loss:  0.6079500913619995\n",
      "epoch:  753  loss:  0.6077325344085693\n",
      "epoch:  754  loss:  0.6075151562690735\n",
      "epoch:  755  loss:  0.6072977781295776\n",
      "epoch:  756  loss:  0.6070806384086609\n",
      "epoch:  757  loss:  0.6068634986877441\n",
      "epoch:  758  loss:  0.6066460013389587\n",
      "epoch:  759  loss:  0.6064286231994629\n",
      "epoch:  760  loss:  0.6062113642692566\n",
      "epoch:  761  loss:  0.6059942841529846\n",
      "epoch:  762  loss:  0.6057772636413574\n",
      "epoch:  763  loss:  0.6055603623390198\n",
      "epoch:  764  loss:  0.6053436398506165\n",
      "epoch:  765  loss:  0.6051269769668579\n",
      "epoch:  766  loss:  0.6049104928970337\n",
      "epoch:  767  loss:  0.604694128036499\n",
      "epoch:  768  loss:  0.6044778227806091\n",
      "epoch:  769  loss:  0.6042616963386536\n",
      "epoch:  770  loss:  0.6040456295013428\n",
      "epoch:  771  loss:  0.6038297414779663\n",
      "epoch:  772  loss:  0.6036139726638794\n",
      "epoch:  773  loss:  0.6033983826637268\n",
      "epoch:  774  loss:  0.6031827926635742\n",
      "epoch:  775  loss:  0.602967381477356\n",
      "epoch:  776  loss:  0.602752149105072\n",
      "epoch:  777  loss:  0.6025369763374329\n",
      "epoch:  778  loss:  0.6023219227790833\n",
      "epoch:  779  loss:  0.6021069884300232\n",
      "epoch:  780  loss:  0.6018921732902527\n",
      "epoch:  781  loss:  0.6016774773597717\n",
      "epoch:  782  loss:  0.6014630198478699\n",
      "epoch:  783  loss:  0.601248562335968\n",
      "epoch:  784  loss:  0.6010341644287109\n",
      "epoch:  785  loss:  0.600820004940033\n",
      "epoch:  786  loss:  0.6006059646606445\n",
      "epoch:  787  loss:  0.6003919839859009\n",
      "epoch:  788  loss:  0.6001781821250916\n",
      "epoch:  789  loss:  0.599964439868927\n",
      "epoch:  790  loss:  0.5997508764266968\n",
      "epoch:  791  loss:  0.5995374321937561\n",
      "epoch:  792  loss:  0.599324107170105\n",
      "epoch:  793  loss:  0.5991109013557434\n",
      "epoch:  794  loss:  0.5988978147506714\n",
      "epoch:  795  loss:  0.5986847877502441\n",
      "epoch:  796  loss:  0.5984719395637512\n",
      "epoch:  797  loss:  0.5982592105865479\n",
      "epoch:  798  loss:  0.598046600818634\n",
      "epoch:  799  loss:  0.5978341102600098\n",
      "epoch:  800  loss:  0.597621738910675\n",
      "epoch:  801  loss:  0.5974094867706299\n",
      "epoch:  802  loss:  0.597197413444519\n",
      "epoch:  803  loss:  0.5969853401184082\n",
      "epoch:  804  loss:  0.5967735052108765\n",
      "epoch:  805  loss:  0.5965616703033447\n",
      "epoch:  806  loss:  0.5963500142097473\n",
      "epoch:  807  loss:  0.5961385369300842\n",
      "epoch:  808  loss:  0.5959271192550659\n",
      "epoch:  809  loss:  0.5957157611846924\n",
      "epoch:  810  loss:  0.5955045819282532\n",
      "epoch:  811  loss:  0.5952935218811035\n",
      "epoch:  812  loss:  0.5950826406478882\n",
      "epoch:  813  loss:  0.5948718786239624\n",
      "epoch:  814  loss:  0.5946610569953918\n",
      "epoch:  815  loss:  0.5944505333900452\n",
      "epoch:  816  loss:  0.5942400693893433\n",
      "epoch:  817  loss:  0.5940297245979309\n",
      "epoch:  818  loss:  0.5938194990158081\n",
      "epoch:  819  loss:  0.5936094522476196\n",
      "epoch:  820  loss:  0.5933994650840759\n",
      "epoch:  821  loss:  0.5931895971298218\n",
      "epoch:  822  loss:  0.5929797887802124\n",
      "epoch:  823  loss:  0.5927702188491821\n",
      "epoch:  824  loss:  0.5925607681274414\n",
      "epoch:  825  loss:  0.5923513174057007\n",
      "epoch:  826  loss:  0.5921420454978943\n",
      "epoch:  827  loss:  0.5919329524040222\n",
      "epoch:  828  loss:  0.5917239189147949\n",
      "epoch:  829  loss:  0.5915149450302124\n",
      "epoch:  830  loss:  0.591306209564209\n",
      "epoch:  831  loss:  0.5910975337028503\n",
      "epoch:  832  loss:  0.590889036655426\n",
      "epoch:  833  loss:  0.5906804800033569\n",
      "epoch:  834  loss:  0.5904721617698669\n",
      "epoch:  835  loss:  0.5902639627456665\n",
      "epoch:  836  loss:  0.5900559425354004\n",
      "epoch:  837  loss:  0.589847981929779\n",
      "epoch:  838  loss:  0.5896401405334473\n",
      "epoch:  839  loss:  0.5894323587417603\n",
      "epoch:  840  loss:  0.5892248153686523\n",
      "epoch:  841  loss:  0.5890172719955444\n",
      "epoch:  842  loss:  0.5888099074363708\n",
      "epoch:  843  loss:  0.588602602481842\n",
      "epoch:  844  loss:  0.5883954167366028\n",
      "epoch:  845  loss:  0.5881884694099426\n",
      "epoch:  846  loss:  0.5879815220832825\n",
      "epoch:  847  loss:  0.5877747535705566\n",
      "epoch:  848  loss:  0.5875680446624756\n",
      "epoch:  849  loss:  0.5873615145683289\n",
      "epoch:  850  loss:  0.5871550440788269\n",
      "epoch:  851  loss:  0.5869487524032593\n",
      "epoch:  852  loss:  0.5867425799369812\n",
      "epoch:  853  loss:  0.5865364670753479\n",
      "epoch:  854  loss:  0.5863304734230042\n",
      "epoch:  855  loss:  0.5861246585845947\n",
      "epoch:  856  loss:  0.5859188437461853\n",
      "epoch:  857  loss:  0.5857132077217102\n",
      "epoch:  858  loss:  0.5855077505111694\n",
      "epoch:  859  loss:  0.5853029489517212\n",
      "epoch:  860  loss:  0.5850982666015625\n",
      "epoch:  861  loss:  0.584894061088562\n",
      "epoch:  862  loss:  0.5846900343894958\n",
      "epoch:  863  loss:  0.5844860672950745\n",
      "epoch:  864  loss:  0.5842822194099426\n",
      "epoch:  865  loss:  0.5840784311294556\n",
      "epoch:  866  loss:  0.5838748216629028\n",
      "epoch:  867  loss:  0.5836713314056396\n",
      "epoch:  868  loss:  0.583467960357666\n",
      "epoch:  869  loss:  0.5832647085189819\n",
      "epoch:  870  loss:  0.5830615758895874\n",
      "epoch:  871  loss:  0.5828584432601929\n",
      "epoch:  872  loss:  0.5826555490493774\n",
      "epoch:  873  loss:  0.582452654838562\n",
      "epoch:  874  loss:  0.5822499990463257\n",
      "epoch:  875  loss:  0.5820474624633789\n",
      "epoch:  876  loss:  0.5818449854850769\n",
      "epoch:  877  loss:  0.5816425681114197\n",
      "epoch:  878  loss:  0.5814403891563416\n",
      "epoch:  879  loss:  0.5812382698059082\n",
      "epoch:  880  loss:  0.5810362100601196\n",
      "epoch:  881  loss:  0.5808342695236206\n",
      "epoch:  882  loss:  0.5806324481964111\n",
      "epoch:  883  loss:  0.5804307460784912\n",
      "epoch:  884  loss:  0.5802292227745056\n",
      "epoch:  885  loss:  0.5800277590751648\n",
      "epoch:  886  loss:  0.5798264145851135\n",
      "epoch:  887  loss:  0.5796251893043518\n",
      "epoch:  888  loss:  0.5794240832328796\n",
      "epoch:  889  loss:  0.5792230367660522\n",
      "epoch:  890  loss:  0.579022228717804\n",
      "epoch:  891  loss:  0.5788214802742004\n",
      "epoch:  892  loss:  0.5786207318305969\n",
      "epoch:  893  loss:  0.5784202218055725\n",
      "epoch:  894  loss:  0.5782197713851929\n",
      "epoch:  895  loss:  0.5780194401741028\n",
      "epoch:  896  loss:  0.5778192281723022\n",
      "epoch:  897  loss:  0.5776191353797913\n",
      "epoch:  898  loss:  0.5774191617965698\n",
      "epoch:  899  loss:  0.5772193074226379\n",
      "epoch:  900  loss:  0.5770195722579956\n",
      "epoch:  901  loss:  0.5768199563026428\n",
      "epoch:  902  loss:  0.5766203999519348\n",
      "epoch:  903  loss:  0.5764209628105164\n",
      "epoch:  904  loss:  0.5762216448783875\n",
      "epoch:  905  loss:  0.5760224461555481\n",
      "epoch:  906  loss:  0.5758234262466431\n",
      "epoch:  907  loss:  0.5756244659423828\n",
      "epoch:  908  loss:  0.5754255652427673\n",
      "epoch:  909  loss:  0.5752267837524414\n",
      "epoch:  910  loss:  0.5750281810760498\n",
      "epoch:  911  loss:  0.5748294591903687\n",
      "epoch:  912  loss:  0.5746307969093323\n",
      "epoch:  913  loss:  0.5744321346282959\n",
      "epoch:  914  loss:  0.5742335915565491\n",
      "epoch:  915  loss:  0.5740351676940918\n",
      "epoch:  916  loss:  0.5738369226455688\n",
      "epoch:  917  loss:  0.5736386775970459\n",
      "epoch:  918  loss:  0.573440670967102\n",
      "epoch:  919  loss:  0.5732426643371582\n",
      "epoch:  920  loss:  0.5730447173118591\n",
      "epoch:  921  loss:  0.5728470683097839\n",
      "epoch:  922  loss:  0.5726494193077087\n",
      "epoch:  923  loss:  0.5724518895149231\n",
      "epoch:  924  loss:  0.5722545385360718\n",
      "epoch:  925  loss:  0.5720571875572205\n",
      "epoch:  926  loss:  0.5718600153923035\n",
      "epoch:  927  loss:  0.5716629028320312\n",
      "epoch:  928  loss:  0.5714659690856934\n",
      "epoch:  929  loss:  0.5712690949440002\n",
      "epoch:  930  loss:  0.5710723996162415\n",
      "epoch:  931  loss:  0.5708757638931274\n",
      "epoch:  932  loss:  0.5706791877746582\n",
      "epoch:  933  loss:  0.5704828500747681\n",
      "epoch:  934  loss:  0.5702865719795227\n",
      "epoch:  935  loss:  0.5700903534889221\n",
      "epoch:  936  loss:  0.5698942542076111\n",
      "epoch:  937  loss:  0.5696982741355896\n",
      "epoch:  938  loss:  0.5695023536682129\n",
      "epoch:  939  loss:  0.5693066716194153\n",
      "epoch:  940  loss:  0.5691110491752625\n",
      "epoch:  941  loss:  0.5689155459403992\n",
      "epoch:  942  loss:  0.5687201023101807\n",
      "epoch:  943  loss:  0.5685248374938965\n",
      "epoch:  944  loss:  0.5683295726776123\n",
      "epoch:  945  loss:  0.5681344866752625\n",
      "epoch:  946  loss:  0.5679394602775574\n",
      "epoch:  947  loss:  0.5677446126937866\n",
      "epoch:  948  loss:  0.5675498843193054\n",
      "epoch:  949  loss:  0.5673552751541138\n",
      "epoch:  950  loss:  0.5671607255935669\n",
      "epoch:  951  loss:  0.5669662952423096\n",
      "epoch:  952  loss:  0.5667719841003418\n",
      "epoch:  953  loss:  0.5665777325630188\n",
      "epoch:  954  loss:  0.5663836598396301\n",
      "epoch:  955  loss:  0.5661896467208862\n",
      "epoch:  956  loss:  0.5659957528114319\n",
      "epoch:  957  loss:  0.5658019781112671\n",
      "epoch:  958  loss:  0.5656082630157471\n",
      "epoch:  959  loss:  0.5654147863388062\n",
      "epoch:  960  loss:  0.5652213096618652\n",
      "epoch:  961  loss:  0.5650280117988586\n",
      "epoch:  962  loss:  0.5648347735404968\n",
      "epoch:  963  loss:  0.5646417140960693\n",
      "epoch:  964  loss:  0.5644486546516418\n",
      "epoch:  965  loss:  0.5642557144165039\n",
      "epoch:  966  loss:  0.5640628337860107\n",
      "epoch:  967  loss:  0.5638700723648071\n",
      "epoch:  968  loss:  0.5636773705482483\n",
      "epoch:  969  loss:  0.5634848475456238\n",
      "epoch:  970  loss:  0.563292384147644\n",
      "epoch:  971  loss:  0.5630999803543091\n",
      "epoch:  972  loss:  0.562907874584198\n",
      "epoch:  973  loss:  0.5627157092094421\n",
      "epoch:  974  loss:  0.5625237226486206\n",
      "epoch:  975  loss:  0.5623317956924438\n",
      "epoch:  976  loss:  0.5621400475502014\n",
      "epoch:  977  loss:  0.561948299407959\n",
      "epoch:  978  loss:  0.5617567300796509\n",
      "epoch:  979  loss:  0.5615652799606323\n",
      "epoch:  980  loss:  0.5613739490509033\n",
      "epoch:  981  loss:  0.5611826777458191\n",
      "epoch:  982  loss:  0.5609915256500244\n",
      "epoch:  983  loss:  0.5608004927635193\n",
      "epoch:  984  loss:  0.5606095194816589\n",
      "epoch:  985  loss:  0.5604187250137329\n",
      "epoch:  986  loss:  0.5602280497550964\n",
      "epoch:  987  loss:  0.56003737449646\n",
      "epoch:  988  loss:  0.5598468780517578\n",
      "epoch:  989  loss:  0.5596565008163452\n",
      "epoch:  990  loss:  0.5594661831855774\n",
      "epoch:  991  loss:  0.5592761039733887\n",
      "epoch:  992  loss:  0.5590859651565552\n",
      "epoch:  993  loss:  0.558896005153656\n",
      "epoch:  994  loss:  0.5587061047554016\n",
      "epoch:  995  loss:  0.5585163831710815\n",
      "epoch:  996  loss:  0.558326780796051\n",
      "epoch:  997  loss:  0.5581372380256653\n",
      "epoch:  998  loss:  0.5579477548599243\n",
      "epoch:  999  loss:  0.5577584505081177\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    y_pred_3 = model_lr_1(X)\n",
    "    loss = metrics_name_2(y_pred_3, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    Optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    Optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b53559e8-5a02-4cee-b6b2-1990e714389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3b813bf4-458d-4c5a-b18e-cdf5297fb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1(model_lr_1, loss_fn, optimizer):\n",
    "    \"\"\"function_1 train_1\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    model_lr_1.train()\n",
    "    for batch_size_lr_1 in range(1000):\n",
    "        y_pred_4 = model_lr_1(X)\n",
    "        loss = loss_fn(y_pred_4, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if BATCH_SIZE_LR_1 % 16 == 0:\n",
    "            loss_3 = loss.item(), BATCH_SIZE_LR * len(X)\n",
    "            print(f'loss: {loss_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fb4aed1a-5bc3-449a-80ad-5d9c15cce36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function_1 train_1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f4d1844b-cb04-4697-9edd-c310a616f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1(model_lr_1, loss_fn):\n",
    "    \"\"\"function_1 test_1\"\"\"\n",
    "    size = len(X_train_tensor)\n",
    "    num_batches = size\n",
    "    model_lr_1.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_size_lr_1 in range(1000):\n",
    "            y_pred_5 = model_lr_1(X)\n",
    "            test_loss += loss_fn(y_pred_5, y).item()\n",
    "            test_loss /= num_batches\n",
    "            print(f'Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5b6d9f8b-950d-4f44-afcb-40bd6842e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"function_1 test_1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "43a88f18-61e1-4206-9d7f-bb78c7665089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0544]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_lr_1(X_test_tensor[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8dfedb40-ca9d-4016-bd5f-0e3544ad1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_tensor[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "05f9c27f-34ec-4ef8-aa32-ca3b433289bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_lr_1, \"lr_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f202dec1-32a9-4f82-8ffe-4d28254b3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_lr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71037f19-c0b0-41ef-b7a6-a04ae84ebd35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
